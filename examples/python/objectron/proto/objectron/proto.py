# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: a_r_capture_metadata.proto, object.proto, annotation_data.proto
# plugin: python-betterproto
from __future__ import annotations

from dataclasses import dataclass
from typing import List

import betterproto


class AVDepthDataAccuracy(betterproto.Enum):
    UNDEFINED_ACCURACY = 0
    RELATIVE = 1
    ABSOLUTE = 2


class AVDepthDataQuality(betterproto.Enum):
    UNDEFINED_QUALITY = 0
    HIGH = 1
    LOW = 2


class ARCameraTrackingState(betterproto.Enum):
    UNDEFINED_TRACKING_STATE = 0
    UNAVAILABLE = 1
    LIMITED = 2
    NORMAL = 3


class ARCameraTrackingStateReason(betterproto.Enum):
    UNDEFINED_TRACKING_STATE_REASON = 0
    NONE = 1
    INITIALIZING = 2
    EXCESSIVE_MOTION = 3
    INSUFFICIENT_FEATURES = 4
    RELOCALIZING = 5


class ARPlaneAnchorAlignment(betterproto.Enum):
    UNDEFINED = 0
    HORIZONTAL = 1
    VERTICAL = 2


class ARPlaneAnchorPlaneClassification(betterproto.Enum):
    NONE = 0
    WALL = 1
    FLOOR = 2
    CEILING = 3
    TABLE = 4
    SEAT = 5


class ARPlaneAnchorPlaneClassificationStatus(betterproto.Enum):
    UNKNOWN = 0
    UNAVAILABLE = 1
    UNDETERMINED = 2
    KNOWN = 3


class CMCalibratedMagneticFieldCalibrationAccuracy(betterproto.Enum):
    UNCALIBRATED = 0
    LOW = 1
    MEDIUM = 2
    HIGH = 3


class ARMeshGeometryMeshClassification(betterproto.Enum):
    NONE = 0
    WALL = 1
    FLOOR = 2
    CEILING = 3
    TABLE = 4
    SEAT = 5
    WINDOW = 6
    DOOR = 7


class ObjectType(betterproto.Enum):
    UNDEFINED_TYPE = 0
    BOUNDING_BOX = 1
    SKELETON = 2
    MESH = 3


class ObjectMethod(betterproto.Enum):
    UNKNOWN_METHOD = 0
    ANNOTATION = 1
    AUGMENTATION = 2


@dataclass
class AVCameraCalibrationData(betterproto.Message):
    """
    Info about the camera characteristics used to capture images and depth
    data.
    """

    # 3x3 row-major matrix relating a camera's internal properties to an ideal
    # pinhole-camera model.
    intrinsic_matrix: list[float] = betterproto.float_field(1)
    # The image dimensions to which the intrinsic_matrix values are relative.
    intrinsic_matrix_reference_dimension_width: float = betterproto.float_field(2)
    intrinsic_matrix_reference_dimension_height: float = betterproto.float_field(3)
    # 3x4 row-major matrix relating a camera's position and orientation to a
    # world or scene coordinate system. Consists of a unitless 3x3 rotation
    # matrix (R) on the left and a translation (t) 3x1 vector on the right. The
    # translation vector's units are millimeters. For example:            |r1,1
    # r2,1  r3,1 | t1|  [R | t] = |r1,2  r2,2  r3,2 | t2|            |r1,3  r2,3
    # r3,3 | t3|  is stored as [r11, r21, r31, t1, r12, r22, r32, t2, ...]
    extrinsic_matrix: list[float] = betterproto.float_field(4)
    # The size, in millimeters, of one image pixel.
    pixel_size: float = betterproto.float_field(5)
    # A list of floating-point values describing radial distortions imparted by
    # the camera lens, for use in rectifying camera images.
    lens_distortion_lookup_values: list[float] = betterproto.float_field(6)
    # A list of floating-point values describing radial distortions for use in
    # reapplying camera geometry to a rectified image.
    inverse_lens_distortion_lookup_values: list[float] = betterproto.float_field(7)
    # The offset of the distortion center of the camera lens from the top-left
    # corner of the image.
    lens_distortion_center_x: float = betterproto.float_field(8)
    lens_distortion_center_y: float = betterproto.float_field(9)


@dataclass
class AVDepthData(betterproto.Message):
    """Container for depth data information."""

    # PNG representation of the grayscale depth data map. See discussion about
    # depth_data_map_original_minimum_value, below, for information about how to
    # interpret the pixel values.
    depth_data_map: bytes = betterproto.bytes_field(1)
    # Pixel format type of the original captured depth data.
    depth_data_type: str = betterproto.string_field(2)
    depth_data_accuracy: AVDepthDataAccuracy = betterproto.enum_field(3)
    # Indicates whether the depth_data_map contains temporally smoothed data.
    depth_data_filtered: bool = betterproto.bool_field(4)
    depth_data_quality: AVDepthDataQuality = betterproto.enum_field(5)
    # Associated calibration data for the depth_data_map.
    camera_calibration_data: AVCameraCalibrationData = betterproto.message_field(6)
    # The original range of values expressed by the depth_data_map, before
    # grayscale normalization. For example, if the minimum and maximum values
    # indicate a range of [0.5, 2.2], and the depth_data_type value indicates it
    # was a depth map, then white pixels (255, 255, 255) will map to 0.5 and
    # black pixels (0, 0, 0) will map to 2.2 with the grayscale range linearly
    # interpolated inbetween. Conversely, if the depth_data_type value indicates
    # it was a disparity map, then white pixels will map to 2.2 and black pixels
    # will map to 0.5.
    depth_data_map_original_minimum_value: float = betterproto.float_field(7)
    depth_data_map_original_maximum_value: float = betterproto.float_field(8)
    # The width of the depth buffer map.
    depth_data_map_width: int = betterproto.int32_field(9)
    # The height of the depth buffer map.
    depth_data_map_height: int = betterproto.int32_field(10)
    # The row-major flattened array of the depth buffer map pixels. This will be
    # either a float32 or float16 byte array, depending on 'depth_data_type'.
    depth_data_map_raw_values: bytes = betterproto.bytes_field(11)


@dataclass
class ARLightEstimate(betterproto.Message):
    """
    Estimated scene lighting information associated with a captured video
    frame.
    """

    # The estimated intensity, in lumens, of ambient light throughout the scene.
    ambient_intensity: float = betterproto.double_field(1)
    # The estimated color temperature, in degrees Kelvin, of ambient light
    # throughout the scene.
    ambient_color_temperature: float = betterproto.double_field(2)
    # Data describing the estimated lighting environment in all directions.
    # Second-level spherical harmonics in separate red, green, and blue data
    # planes. Thus, this buffer contains 3 sets of 9 coefficients, or a total of
    # 27 values.
    spherical_harmonics_coefficients: list[float] = betterproto.float_field(3)
    # A vector indicating the orientation of the strongest directional light
    # source, normalized in the world-coordinate space.
    primary_light_direction: ARLightEstimateDirectionVector = betterproto.message_field(4)
    # The estimated intensity, in lumens, of the strongest directional light
    # source in the scene.
    primary_light_intensity: float = betterproto.float_field(5)


@dataclass
class ARLightEstimateDirectionVector(betterproto.Message):
    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)


@dataclass
class ARCamera(betterproto.Message):
    """
    Information about the camera position and imaging characteristics for a
    captured video frame.
    """

    tracking_state: ARCameraTrackingState = betterproto.enum_field(1)
    tracking_state_reason: ARCameraTrackingStateReason = betterproto.enum_field(2)
    # 4x4 row-major matrix expressing position and orientation of the camera in
    # world coordinate space.
    transform: list[float] = betterproto.float_field(3)
    euler_angles: ARCameraEulerAngles = betterproto.message_field(4)
    # The width and height, in pixels, of the captured camera image.
    image_resolution_width: int = betterproto.int32_field(5)
    image_resolution_height: int = betterproto.int32_field(6)
    # 3x3 row-major matrix that converts between the 2D camera plane and 3D world
    # coordinate space.
    intrinsics: list[float] = betterproto.float_field(7)
    # 4x4 row-major transform matrix appropriate for rendering 3D content to
    # match the image captured by the camera.
    projection_matrix: list[float] = betterproto.float_field(8)
    # 4x4 row-major transform matrix appropriate for converting from world-space
    # to camera space. Relativized for the captured_image orientation (i.e.
    # UILandscapeOrientationRight).
    view_matrix: list[float] = betterproto.float_field(9)


@dataclass
class ARCameraEulerAngles(betterproto.Message):
    """
    The orientation of the camera, expressed as roll, pitch, and yaw values.
    """

    roll: float = betterproto.float_field(1)
    pitch: float = betterproto.float_field(2)
    yaw: float = betterproto.float_field(3)


@dataclass
class ARFaceGeometry(betterproto.Message):
    """Container for a 3D mesh describing face topology."""

    vertices: list[ARFaceGeometryVertex] = betterproto.message_field(1)
    # The number of elements in the vertices list.
    vertex_count: int = betterproto.int32_field(2)
    texture_coordinates: list[ARFaceGeometryTextureCoordinate] = betterproto.message_field(3)
    # The number of elements in the texture_coordinates list.
    texture_coordinate_count: int = betterproto.int32_field(4)
    # Each integer value in this ordered list represents an index into the
    # vertices and texture_coordinates lists. Each set of three indices
    # identifies the vertices comprising a single triangle in the mesh. Each set
    # of three indices forms a triangle, so the number of indices in the
    # triangle_indices buffer is three times the triangle_count value.
    triangle_indices: list[int] = betterproto.int32_field(5)
    # The number of triangles described by the triangle_indices buffer.
    triangle_count: int = betterproto.int32_field(6)


@dataclass
class ARFaceGeometryVertex(betterproto.Message):
    """
    Each vertex represents a 3D point in the face mesh, in the face coordinate
    space.
    """

    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)


@dataclass
class ARFaceGeometryTextureCoordinate(betterproto.Message):
    """
    Each texture coordinate represents UV texture coordinates for the vertex at
    the corresponding index in the vertices buffer.
    """

    u: float = betterproto.float_field(1)
    v: float = betterproto.float_field(2)


@dataclass
class ARBlendShapeMap(betterproto.Message):
    """
    Contains a list of blend shape entries wherein each item maps a specific
    blend shape location to its associated coefficient.
    """

    entries: list[ARBlendShapeMapMapEntry] = betterproto.message_field(1)


@dataclass
class ARBlendShapeMapMapEntry(betterproto.Message):
    # Identifier for the specific facial feature.
    blend_shape_location: str = betterproto.string_field(1)
    # Indicates the current position of the feature relative to its neutral
    # configuration, ranging from 0.0 (neutral) to 1.0 (maximum movement).
    blend_shape_coefficient: float = betterproto.float_field(2)


@dataclass
class ARFaceAnchor(betterproto.Message):
    """
    Information about the pose, topology, and expression of a detected face.
    """

    # A coarse triangle mesh representing the topology of the detected face.
    geometry: ARFaceGeometry = betterproto.message_field(1)
    # A map of named coefficients representing the detected facial expression in
    # terms of the movement of specific facial features.
    blend_shapes: ARBlendShapeMap = betterproto.message_field(2)
    # 4x4 row-major matrix encoding the position, orientation, and scale of the
    # anchor relative to the world coordinate space.
    transform: list[float] = betterproto.float_field(3)
    # Indicates whether the anchor's transform is valid. Frames that have a face
    # anchor with this value set to NO should probably be ignored.
    is_tracked: bool = betterproto.bool_field(4)


@dataclass
class ARPlaneGeometry(betterproto.Message):
    """Container for a 3D mesh."""

    # A buffer of vertex positions for each point in the plane mesh.
    vertices: list[ARPlaneGeometryVertex] = betterproto.message_field(1)
    # The number of elements in the vertices buffer.
    vertex_count: int = betterproto.int32_field(2)
    # A buffer of texture coordinate values for each point in the plane mesh.
    texture_coordinates: list[ARPlaneGeometryTextureCoordinate] = betterproto.message_field(3)
    # The number of elements in the texture_coordinates buffer.
    texture_coordinate_count: int = betterproto.int32_field(4)
    # Each integer value in this ordered list represents an index into the
    # vertices and texture_coordinates lists. Each set of three indices
    # identifies the vertices comprising a single triangle in the mesh. Each set
    # of three indices forms a triangle, so the number of indices in the
    # triangle_indices buffer is three times the triangle_count value.
    triangle_indices: list[int] = betterproto.int32_field(5)
    # Each set of three indices forms a triangle, so the number of indices in the
    # triangle_indices buffer is three times the triangle_count value.
    triangle_count: int = betterproto.int32_field(6)
    # Each value in this buffer represents the position of a vertex along the
    # boundary polygon of the estimated plane. The owning plane anchor's
    # transform matrix defines the coordinate system for these points.
    boundary_vertices: list[ARPlaneGeometryVertex] = betterproto.message_field(7)
    # The number of elements in the boundary_vertices buffer.
    boundary_vertex_count: int = betterproto.int32_field(8)


@dataclass
class ARPlaneGeometryVertex(betterproto.Message):
    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)


@dataclass
class ARPlaneGeometryTextureCoordinate(betterproto.Message):
    """
    Each texture coordinate represents UV texture coordinates for the vertex at
    the corresponding index in the vertices buffer.
    """

    u: float = betterproto.float_field(1)
    v: float = betterproto.float_field(2)


@dataclass
class ARPlaneAnchor(betterproto.Message):
    """
    Information about the position and orientation of a real-world flat
    surface.
    """

    # The ID of the plane.
    identifier: str = betterproto.string_field(1)
    # 4x4 row-major matrix encoding the position, orientation, and scale of the
    # anchor relative to the world coordinate space.
    transform: list[float] = betterproto.float_field(2)
    # The general orientation of the detected plane with respect to gravity.
    alignment: ARPlaneAnchorAlignment = betterproto.enum_field(3)
    # A coarse triangle mesh representing the general shape of the detected
    # plane.
    geometry: ARPlaneGeometry = betterproto.message_field(4)
    # The center point of the plane relative to its anchor position. Although the
    # type of this property is a 3D vector, a plane anchor is always two-
    # dimensional, and is always positioned in only the x and z directions
    # relative to its transform position. (That is, the y-component of this
    # vector is always zero.)
    center: ARPlaneAnchorPlaneVector = betterproto.message_field(5)
    # The estimated width and length of the detected plane.
    extent: ARPlaneAnchorPlaneVector = betterproto.message_field(6)
    # A Boolean value that indicates whether plane classification is available on
    # the current device. On devices without plane classification support, all
    # plane anchors report a classification value of NONE and a
    # classification_status value of UNAVAILABLE.
    classification_supported: bool = betterproto.bool_field(7)
    # A general characterization of what kind of real-world surface the plane
    # anchor represents.
    classification: ARPlaneAnchorPlaneClassification = betterproto.enum_field(8)
    # The current state of process for classifying the plane anchor. When this
    # property's value is KNOWN, the classification property represents
    # characterization of the real-world surface corresponding to the plane
    # anchor.
    classification_status: ARPlaneAnchorPlaneClassificationStatus = betterproto.enum_field(9)


@dataclass
class ARPlaneAnchorPlaneVector(betterproto.Message):
    """
    Wrapper for a 3D point / vector within the plane. See extent and center
    values for more information.
    """

    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)


@dataclass
class ARPointCloud(betterproto.Message):
    """A collection of points in the world coordinate space."""

    # The number of points in the cloud.
    count: int = betterproto.int32_field(1)
    # The list of detected points.
    point: list[ARPointCloudPoint] = betterproto.message_field(2)
    # A list of unique identifiers corresponding to detected feature points. Each
    # identifier in this list corresponds to the point at the same index in the
    # points array.
    identifier: list[int] = betterproto.int64_field(3)


@dataclass
class ARPointCloudPoint(betterproto.Message):
    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)


@dataclass
class CMVector(betterproto.Message):
    """A 3D vector"""

    x: float = betterproto.double_field(1)
    y: float = betterproto.double_field(2)
    z: float = betterproto.double_field(3)


@dataclass
class CMCalibratedMagneticField(betterproto.Message):
    """
    Represents calibrated magnetic field data and accuracy estimate of it.
    """

    # Vector of magnetic field estimate.
    field: CMVector = betterproto.message_field(1)
    # Calibration accuracy of a magnetic field estimate.
    calibration_accuracy: CMCalibratedMagneticFieldCalibrationAccuracy = betterproto.enum_field(2)


@dataclass
class CMDeviceMotion(betterproto.Message):
    """
    A sample of device motion data. Encapsulates measurements of the attitude,
    rotation rate, magnetic field, and acceleration of the device. Core Media
    applies different algorithms of bias-reduction and stabilization to
    rotation rate, magnetic field and acceleration values. For raw values check
    correspondent fields in CMMotionManagerSnapshot object.
    """

    # The device motion data object timestamp. May differ from the frame
    # timestamp value since the data may be collected at higher rate.
    timestamp: float = betterproto.double_field(1)
    # The quaternion representing the device’s orientation relative to a known
    # frame of reference at a point in time.
    attitude_quaternion: CMDeviceMotionQuaternion = betterproto.message_field(2)
    # The gravity acceleration vector expressed in the device's reference frame.
    gravity: CMVector = betterproto.message_field(3)
    # The acceleration that the user is giving to the device.
    user_acceleration: CMVector = betterproto.message_field(4)
    # Returns the magnetic field vector filtered with respect to the device bias.
    magnetic_field: CMCalibratedMagneticField = betterproto.message_field(5)
    # The rotation rate of the device adjusted by bias-removing Core Motion
    # algorithms.
    rotation_rate: CMVector = betterproto.message_field(6)


@dataclass
class CMDeviceMotionQuaternion(betterproto.Message):
    x: float = betterproto.double_field(1)
    y: float = betterproto.double_field(2)
    z: float = betterproto.double_field(3)
    w: float = betterproto.double_field(4)


@dataclass
class CMAccelerometerData(betterproto.Message):
    """A sample of raw accelerometer data."""

    # The accelerometer data object timestamp. May differ from the frame
    # timestamp value since the data may be collected at higher rate.
    timestamp: float = betterproto.double_field(1)
    # Raw acceleration measured by the accelerometer which effectively is a sum
    # of gravity and user_acceleration of CMDeviceMotion object.
    acceleration: CMVector = betterproto.message_field(2)


@dataclass
class CMGyroData(betterproto.Message):
    """A sample of raw gyroscope data."""

    # The gyroscope data object timestamp. May differ from the frame timestamp
    # value since the data may be collected at higher rate.
    timestamp: float = betterproto.double_field(1)
    # Raw rotation rate as measured by the gyroscope.
    rotation_rate: CMVector = betterproto.message_field(2)


@dataclass
class CMMagnetometerData(betterproto.Message):
    """A sample of raw magnetometer data."""

    # The magnetometer data object timestamp. May differ from the frame timestamp
    # value since the data may be collected at higher rate.
    timestamp: float = betterproto.double_field(1)
    # Raw magnetic field measured by the magnetometer.
    magnetic_field: CMVector = betterproto.message_field(2)


@dataclass
class CMMotionManagerSnapshot(betterproto.Message):
    """Contains most recent snapshots of device motion data"""

    # Most recent samples of device motion data.
    device_motion: list[CMDeviceMotion] = betterproto.message_field(1)
    # Most recent samples of raw accelerometer data.
    accelerometer_data: list[CMAccelerometerData] = betterproto.message_field(2)
    # Most recent samples of raw gyroscope data.
    gyro_data: list[CMGyroData] = betterproto.message_field(3)
    # Most recent samples of raw magnetometer data.
    magnetometer_data: list[CMMagnetometerData] = betterproto.message_field(4)


@dataclass
class ARFrame(betterproto.Message):
    """Video image and face position tracking information."""

    # The timestamp for the frame.
    timestamp: float = betterproto.double_field(1)
    # The depth data associated with the frame. Not all frames have depth data.
    depth_data: AVDepthData = betterproto.message_field(2)
    # The depth data object timestamp associated with the frame. May differ from
    # the frame timestamp value. Is only set when the frame has depth_data.
    depth_data_timestamp: float = betterproto.double_field(3)
    # Camera information associated with the frame.
    camera: ARCamera = betterproto.message_field(4)
    # Light information associated with the frame.
    light_estimate: ARLightEstimate = betterproto.message_field(5)
    # Face anchor information associated with the frame. Not all frames have an
    # active face anchor.
    face_anchor: ARFaceAnchor = betterproto.message_field(6)
    # Plane anchors associated with the frame. Not all frames have a plane
    # anchor. Plane anchors and face anchors are mutually exclusive.
    plane_anchor: list[ARPlaneAnchor] = betterproto.message_field(7)
    # The current intermediate results of the scene analysis used to perform
    # world tracking.
    raw_feature_points: ARPointCloud = betterproto.message_field(8)
    # Snapshot of Core Motion CMMotionManager object containing most recent
    # motion data associated with the frame. Since motion data capture rates can
    # be higher than rates of AR capture, the entities of this object reflect all
    # of the aggregated events which have occurred since the last ARFrame was
    # recorded.
    motion_manager_snapshot: CMMotionManagerSnapshot = betterproto.message_field(9)


@dataclass
class ARMeshGeometry(betterproto.Message):
    """Mesh geometry data stored in an array-based format."""

    # The vertices of the mesh.
    vertices: list[ARMeshGeometryVertex] = betterproto.message_field(1)
    # The faces of the mesh.
    faces: list[ARMeshGeometryFace] = betterproto.message_field(2)
    # Rays that define which direction is outside for each face. Normals contain
    # 'rays that define which direction is outside for each face', in practice
    # the normals count is always identical to vertices count which looks like
    # vertices normals and not faces normals.
    normals: list[ARMeshGeometryVertex] = betterproto.message_field(3)
    # Classification for each face in the mesh.
    classification: list[ARMeshGeometryMeshClassification] = betterproto.enum_field(4)


@dataclass
class ARMeshGeometryVertex(betterproto.Message):
    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)


@dataclass
class ARMeshGeometryFace(betterproto.Message):
    # / Indices of vertices defining the face from correspondent array of parent/
    # message. A typical face is triangular.
    vertex_indices: list[int] = betterproto.int32_field(1)


@dataclass
class ARMeshAnchor(betterproto.Message):
    """
    A subdividision of the reconstructed, real-world scene surrounding the
    user.
    """

    # The ID of the mesh.
    identifier: str = betterproto.string_field(1)
    # 4x4 row-major matrix encoding the position, orientation, and scale of the
    # anchor relative to the world coordinate space.
    transform: list[float] = betterproto.float_field(2)
    # 3D information about the mesh such as its shape and classifications.
    geometry: ARMeshGeometry = betterproto.message_field(3)


@dataclass
class ARMeshData(betterproto.Message):
    """
    Container object for mesh data of real-world scene surrounding the user.
    Even though each ARFrame may have a set of ARMeshAnchors associated with
    it, only a single frame's worth of mesh data is written separately at the
    end of each recording due to concerns regarding latency and memory bloat.
    """

    # The timestamp for the data.
    timestamp: float = betterproto.double_field(1)
    # Set of mesh anchors containing the mesh data.
    mesh_anchor: list[ARMeshAnchor] = betterproto.message_field(2)


@dataclass
class KeyPoint(betterproto.Message):
    # The position of the keypoint in the local coordinate system of the rigid
    # object.
    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)
    # Sphere around the keypoint, indicating annotator's confidence of the
    # position in meters.
    confidence_radius: float = betterproto.float_field(4)
    # The name of the keypoint (e.g. legs, head, etc.). Does not have to be
    # unique.
    name: str = betterproto.string_field(5)
    # Indicates whether the keypoint is hidden or not.
    hidden: bool = betterproto.bool_field(6)


@dataclass
class Object(betterproto.Message):
    # Unique object id through a sequence. There might be multiple objects of the
    # same label in this sequence.
    id: int = betterproto.int32_field(1)
    # Describes what category an object is. E.g. object class, attribute,
    # instance or person identity. This provides additional context for the
    # object type.
    category: str = betterproto.string_field(2)
    type: ObjectType = betterproto.enum_field(3)
    # 3x3 row-major rotation matrix describing the orientation of the rigid
    # object's frame of reference in the world-coordinate system.
    rotation: list[float] = betterproto.float_field(4)
    # 3x1 vector describing the translation of the rigid object's frame of
    # reference in the world-coordinate system in meters.
    translation: list[float] = betterproto.float_field(5)
    # 3x1 vector describing the scale of the rigid object's frame of reference in
    # the world-coordinate system in meters.
    scale: list[float] = betterproto.float_field(6)
    # List of all the key points associated with this object in the object
    # coordinate system. The first keypoint is always the object's frame of
    # reference, e.g. the centroid of the box. E.g. bounding box with its center
    # as frame of reference, the 9 keypoints : {0., 0., 0.}, {-.5, -.5, -.5},
    # {-.5, -.5, +.5}, {-.5, +.5, -.5}, {-.5, +.5, +.5}, {+.5, -.5, -.5}, {+.5,
    # -.5, +.5}, {+.5, +.5, -.5}, {+.5, +.5, +.5} To get the bounding box in the
    # world-coordinate system, we first scale the box then transform the scaled
    # box. For example, bounding box in the world coordinate system is rotation *
    # scale * keypoints + translation
    keypoints: list[KeyPoint] = betterproto.message_field(7)
    method: ObjectMethod = betterproto.enum_field(8)


@dataclass
class Edge(betterproto.Message):
    """The edge connecting two keypoints together"""

    # keypoint id of the edge's source
    source: int = betterproto.int32_field(1)
    # keypoint id of the edge's sink
    sink: int = betterproto.int32_field(2)


@dataclass
class Skeleton(betterproto.Message):
    """
    The skeleton template for different objects (e.g. humans, chairs, hands,
    etc) The annotation tool reads the skeleton template dictionary.
    """

    # The origin keypoint in the object coordinate system. (i.e. Point 0, 0, 0)
    reference_keypoint: int = betterproto.int32_field(1)
    # The skeleton's category (e.g. human, chair, hand.). Should be unique in the
    # dictionary.
    category: str = betterproto.string_field(2)
    # Initialization value for all the keypoints in the skeleton in the object's
    # local coordinate system. Pursuit will transform these points using object's
    # transformation to get the keypoint in the world-coordinate.
    keypoints: list[KeyPoint] = betterproto.message_field(3)
    # List of edges connecting keypoints
    edges: list[Edge] = betterproto.message_field(4)


@dataclass
class Skeletons(betterproto.Message):
    """
    The list of all the modeled skeletons in our library. These models can be
    objects (chairs, desks, etc), humans (full pose, hands, faces, etc), or
    box. We can have multiple skeletons in the same file.
    """

    object: list[Skeleton] = betterproto.message_field(1)


@dataclass
class NormalizedPoint2D(betterproto.Message):
    """Projection of a 3D point on an image, and its metric depth."""

    # x-y position of the 2d keypoint in the image coordinate system. u,v \in [0,
    # 1], where top left corner is (0, 0) and the bottom-right corner is (1, 1).
    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    # The depth of the point in the camera coordinate system (in meters).
    depth: float = betterproto.float_field(3)


@dataclass
class Point3D(betterproto.Message):
    """
    The 3D point in the camera coordinate system, the scales are in meters.
    """

    x: float = betterproto.float_field(1)
    y: float = betterproto.float_field(2)
    z: float = betterproto.float_field(3)


@dataclass
class AnnotatedKeyPoint(betterproto.Message):
    id: int = betterproto.int32_field(1)
    point_3d: Point3D = betterproto.message_field(2)
    point_2d: NormalizedPoint2D = betterproto.message_field(3)


@dataclass
class ObjectAnnotation(betterproto.Message):
    # Reference to the object identifier in ObjectInstance.
    object_id: int = betterproto.int32_field(1)
    # For each objects, list all the annotated keypoints here. E.g. for bounding-
    # boxes, we have 8 keypoints, hands = 21 keypoints, etc. These normalized
    # points are the projection of the Object's 3D keypoint on the current
    # frame's camera poses.
    keypoints: list[AnnotatedKeyPoint] = betterproto.message_field(2)
    # Visibiity of this annotation in a frame.
    visibility: float = betterproto.float_field(3)


@dataclass
class FrameAnnotation(betterproto.Message):
    # Unique frame id, corresponds to images.
    frame_id: int = betterproto.int32_field(1)
    # List of the annotated objects in this frame. Depending on how many object
    # are observable in this frame, we might have non or as much as
    # sequence.objects_size() annotations.
    annotations: list[ObjectAnnotation] = betterproto.message_field(2)
    # Information about the camera transformation (in the world coordinate) and
    # imaging characteristics for a captured video frame.
    camera: ARCamera = betterproto.message_field(3)
    # The timestamp for the frame.
    timestamp: float = betterproto.double_field(4)
    # Plane center and normal in camera frame.
    plane_center: list[float] = betterproto.float_field(5)
    plane_normal: list[float] = betterproto.float_field(6)


@dataclass
class Sequence(betterproto.Message):
    """
    The sequence protocol contains the annotation data for the entire video clip.
    """

    # List of all the annotated 3D objects in this sequence in the world
    # Coordinate system. Given the camera poses of each frame (also in the world-
    # coordinate) these objects bounding boxes can be projected to each frame to
    # get the per-frame annotation (i.e. image_annotation below).
    objects: list[Object] = betterproto.message_field(1)
    # List of annotated data per each frame in sequence + frame information.
    frame_annotations: list[FrameAnnotation] = betterproto.message_field(2)
