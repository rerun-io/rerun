<!--[metadata]
title = "Point-E and Shap-E"
source = "https://github.com/rerun-io/point-shap-e"
tags = ["3D", "diffusion", "point", "mesh"]
thumbnail = "https://static.rerun.io/point-e/5b5beb36dce77d2dac7123b197b825421afcaec0/480w.png"
thumbnail_dimensions = [480, 480]
-->


OpenAI has released two models for text-to-3D generation: Point-E and Shape-E. Both of these methods are fast and interesting but still low fidelity for now.

https://vimeo.com/865974160?autoplay=1&loop=1&autopause=0&background=1&muted=1&ratio=10000:6545

First off, how do these two methods differ from each other? Point-E represents its 3D shapes via point clouds. It does so using a 3-step generation process: first, it generates a single synthetic view using a text-to-image diffusion model (in this case GLIDE).

<picture>
  <source media="(max-width: 480px)" srcset="https://static.rerun.io/pointe-overview/3cbe782935428ccea0473ba0c983947ad1a8a528/480w.png">
  <source media="(max-width: 768px)" srcset="https://static.rerun.io/pointe-overview/3cbe782935428ccea0473ba0c983947ad1a8a528/768w.png">
  <source media="(max-width: 1024px)" srcset="https://static.rerun.io/pointe-overview/3cbe782935428ccea0473ba0c983947ad1a8a528/1024w.png">
  <source media="(max-width: 1200px)" srcset="https://static.rerun.io/pointe-overview/3cbe782935428ccea0473ba0c983947ad1a8a528/1200w.png">
  <img src="https://static.rerun.io/pointe-overview/3cbe782935428ccea0473ba0c983947ad1a8a528/full.png" alt="">
</picture>

It then produces a coarse 3D point cloud using a second diffusion model which conditions on the generated image; third, it generates a fine 3D point cloud using an upsampling network. Finally, a another model is used to predict an SDF from the point cloud, and marching cubes turns it into a mesh. As you can tell, the results arenâ€™t very high quality, but they are fast.

https://vimeo.com/865974180?autoplay=1&loop=1&autopause=0&background=1&muted=1&ratio=10000:6095

Shap-E improves on this by representing 3D shapes implicitly. This is done in two stages. First, an encoder is trained that takes images or a point cloud as input and outputs the weights of a NeRF.

<picture>
  <source media="(max-width: 480px)" srcset="https://static.rerun.io/shape-overview/6e4d96482d9b8b6071f98400b89c2ce202f6be3b/480w.png">
  <source media="(max-width: 768px)" srcset="https://static.rerun.io/shape-overview/6e4d96482d9b8b6071f98400b89c2ce202f6be3b/768w.png">
  <source media="(max-width: 1024px)" srcset="https://static.rerun.io/shape-overview/6e4d96482d9b8b6071f98400b89c2ce202f6be3b/1024w.png">
  <source media="(max-width: 1200px)" srcset="https://static.rerun.io/shape-overview/6e4d96482d9b8b6071f98400b89c2ce202f6be3b/1200w.png">
  <img src="https://static.rerun.io/shape-overview/6e4d96482d9b8b6071f98400b89c2ce202f6be3b/full.png" alt="">
</picture>

In the second stage, a diffusion model is trained on a dataset of NeRF weights generated by the previous encoder. This diffusion model is conditioned on either images or text descriptions. The resulting NeRF also outputs SDF values so that meshes can be extracted using marching cubes again. Here we see the prompt "a cheeseburger" turn into a 3D mesh a set of images.

https://vimeo.com/865974191?autoplay=1&loop=1&autopause=0&background=1&muted=1&ratio=10000:6545

When compared to Point-E on both image-to-mesh and text-to-mesh generation, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.

https://vimeo.com/865974209?autoplay=1&loop=1&autopause=0&background=1&muted=1&ratio=10000:6545

Check out the respective papers to learn more about the details of both methods: "[Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463)" by Heewoo Jun and Alex Nichol; "[Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751)" by Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen.
