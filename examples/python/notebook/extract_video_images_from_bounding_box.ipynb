{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee1aa72-b4ff-4df2-a33e-1fe28e2d2360",
   "metadata": {},
   "source": [
    "# Extract Bounding Box Images from Video\n",
    "\n",
    "This example takes as an input a rrd with a video asset and 2D bounding boxes\n",
    "and extracts the cropped images for the associated frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05010b5f-77e4-4aab-905e-438c05f71e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rerun as rr\n",
    "import rerun.utilities.datafusion.functions as rr_dfn\n",
    "import pyarrow as pa\n",
    "from datafusion import SessionContext, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d54027-025c-45e2-94df-1bc465e80e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, you will need a copy of the rrd from the Detect & Track Example.\n",
    "# Update the file path below accordingly.\n",
    "\n",
    "original_recording = rr.dataframe.load_recording(\"detect_and_track_example.rrd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7b06c-b6e5-45dd-84c1-ca182fbd6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to turn this into a DataFusion DataFrame, we need to create a view\n",
    "# that includes our `latest_at` specification. Otherwise the data for the Pinhole\n",
    "# and the data for the DepthImage will be misaligned.\n",
    "\n",
    "original_recording_view = (\n",
    "    original_recording\n",
    "    .view(index=\"log_time\", contents=\"/**\", include_indicator_columns=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e275c-b39f-4bf8-aed7-43b9a07905f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some constants that will be reused below\n",
    "\n",
    "FRAME_INDEX = \"frame\"\n",
    "INPUT_ENTITY_PATH = \"/video/tracked/14\"\n",
    "INPUT_VIDEO_PATH = \"/video\"\n",
    "OUTPUT_ENTITY_PATH = \"/cropped_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8668530-3017-407c-a8bd-0c3d18cbc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the selection to retrieve only the required data\n",
    "\n",
    "required_input_columns = [\n",
    "    \"frame\",\n",
    "    \"log_tick\",\n",
    "    \"log_time\",\n",
    "    f\"{INPUT_ENTITY_PATH}:Position2D\",\n",
    "    f\"{INPUT_ENTITY_PATH}:HalfSize2D\",\n",
    "    f\"{INPUT_VIDEO_PATH}:Blob\",\n",
    "    f\"{INPUT_VIDEO_PATH}:MediaType\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80583a-27f1-415a-98ad-e55b539c24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFusion context and DataFrame from record batches provided\n",
    "# by the view above.\n",
    "\n",
    "batches = [r for r in original_recording_view.select(*required_input_columns)]\n",
    "ctx = SessionContext()\n",
    "df = ctx.create_dataframe([batches])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ec9ce-f080-4d4a-af49-c90f302ae88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Rerun function to extract the images from the video\n",
    "\n",
    "df_images = rr_dfn.extract_bounding_box_images_from_video(\n",
    "    df,\n",
    "    FRAME_INDEX,\n",
    "    INPUT_ENTITY_PATH,\n",
    "    INPUT_VIDEO_PATH,\n",
    "    OUTPUT_ENTITY_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7912a2b-1726-4925-8f62-1ebfbc61da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the results, create a recording stream\n",
    "\n",
    "local_rec = rr.RecordingStream(\"image_extraction\")\n",
    "local_rec.spawn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f027c-a426-4840-8c2d-0d1dfcc92e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataFrame above will contain the original data as well as the generated images.\n",
    "# We select down to only what we want to send to the viewer.\n",
    "\n",
    "df_images = df_images.select(\n",
    "    \"frame\",\n",
    "    \"log_tick\",\n",
    "    \"log_time\",\n",
    "    f\"{OUTPUT_ENTITY_PATH}:ImageBuffer\",\n",
    "    f\"{OUTPUT_ENTITY_PATH}:ImageFormat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a6423-f800-4b3a-8d3d-e97123f35ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the original data so we can show the new images alongside the original data\n",
    "\n",
    "local_rec.send_recording(original_recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0422ee-5621-4e80-ad82-f539822e02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFusion DataFrame into a pyarrow Table and send it to the viewer\n",
    "\n",
    "table_result = pa.table(df_images)\n",
    "rr.dataframe.send_dataframe(table_result, rec=local_rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
