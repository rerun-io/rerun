{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554b047-8616-4dd9-9494-1d2b22d5e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450fd1f4-65e6-463c-a5ae-18c5b31aeeb7",
   "metadata": {},
   "source": [
    "# Data processing status example\n",
    "\n",
    "The purpose of this example is to demonstrate how one would set up a data flow where you are incrementally\n",
    "processing partitions within a dataset. The general concept is that you have two tables that you will use,\n",
    "one for status and one for results. The purpose of the status table is to have a small table that is easy\n",
    "to query for partitions that have not yet been processed.\n",
    "\n",
    "In this example, we first create these two tables. Then we collect the available partitions and compare them\n",
    "to the status table. To demonstrate how you could batch process a portion of your available data, we simply\n",
    "take a subset of the returned values that are not yet processed. In customer work flows, you will likely\n",
    "want to pass all of the available partitions to work or you might prefer to send off a single partition at\n",
    "a time. The details of how you select which partitions to process are up to the individual workflows.\n",
    "\n",
    "The code below produces a few lines of status output and then displays both the results and status tables.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This example assumes you have started the OSS server using the dataset example located in the test\n",
    "asset directory. From the rerun repository you can start this using the following command.\n",
    "\n",
    "```shell\n",
    "rerun server --dataset ./tests/assets/rrd/dataset\n",
    "```\n",
    "\n",
    "The example below creates a temporary directory. It will not persist after this notebook has been executed,\n",
    "so you will need to restart your server if you want to run the example multiple times. If you would prefer\n",
    "to persist the created table, you can change the remove the `with tempfile.TemporaryDirectory()` line and\n",
    "instead set a specific location for your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f2c20-62c4-426c-842d-2c41d3da49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datafusion import col, functions as F, DataFrame\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from rerun.catalog import CatalogClient, DatasetEntry\n",
    "from typing import List\n",
    "import pyarrow as pa\n",
    "import tempfile\n",
    "\n",
    "CATALOG_URL = \"rerun+http://localhost:51234\"\n",
    "DATASET_NAME = \"dataset\"\n",
    "\n",
    "STATUS_TABLE_NAME = \"status\"\n",
    "RESULTS_TABLE_NAME = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb648a-637a-44c9-82e2-512061d95b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(\n",
    "    client: CatalogClient, directory: Path, table_name: str, schema: pa.Schema\n",
    ") -> DataFrame:\n",
    "    \"\"\"Create a lance table at a specified location and return its DataFrame.\"\"\"\n",
    "    if table_name in client.table_names():\n",
    "        return client.get_table(name=table_name)\n",
    "\n",
    "    url = f\"file://{directory}/{table_name}\"\n",
    "\n",
    "    return client.create_table(table_name, schema, url).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae278a02-fe2d-4efe-b6c5-5214f2c5edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_status_table(client: CatalogClient, directory: Path) -> DataFrame:\n",
    "    \"\"\"Create the status table.\"\"\"\n",
    "    schema = pa.schema(\n",
    "        [\n",
    "            (\"rerun_partition_id\", pa.utf8()),\n",
    "            (\"is_complete\", pa.bool_()),\n",
    "            (\"update_time\", pa.timestamp(unit=\"ms\")),\n",
    "        ]\n",
    "    )\n",
    "    return create_table(client, directory, STATUS_TABLE_NAME, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d529be-018e-434a-9c74-92bd94256298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(client: CatalogClient, directory: Path) -> DataFrame:\n",
    "    \"\"\"Create the results table.\"\"\"\n",
    "    schema = pa.schema(\n",
    "        [\n",
    "            (\"rerun_partition_id\", pa.utf8()),\n",
    "            (\"first_log_time\", pa.timestamp(unit=\"ns\")),\n",
    "            (\"last_log_time\", pa.timestamp(unit=\"ns\")),\n",
    "            (\"first_position_obj1\", pa.list_(pa.float32(), 3)),\n",
    "            (\"first_position_obj2\", pa.list_(pa.float32(), 3)),\n",
    "            (\"first_position_obj3\", pa.list_(pa.float32(), 3)),\n",
    "        ]\n",
    "    )\n",
    "    return create_table(client, directory, RESULTS_TABLE_NAME, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bffdb0-c470-4470-8302-e3f16f3c60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_partitions(\n",
    "    partition_table: DataFrame, status_table: DataFrame\n",
    ") -> List[str]:\n",
    "    \"\"\"Query the status table for partitions that have not processed.\"\"\"\n",
    "    status_table = status_table.filter(col(\"is_complete\"))\n",
    "    partitions = partition_table.join(\n",
    "        status_table, on=\"rerun_partition_id\", how=\"anti\"\n",
    "    ).collect()\n",
    "    return [str(r) for rss in partitions for rs in rss for r in rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd4a67-ff8a-4df7-bd16-a24c68b36920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partitions(\n",
    "    client: CatalogClient, dataset: DatasetEntry, partition_list: list[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Example code for processing some partitions within a dataset.\n",
    "\n",
    "    This example performs a simple aggregation of some of the values stored in the dataset that\n",
    "    might be useful for further processing or metrics extraction. In this work flow we first write\n",
    "    to the status table that we have started work but set the `is_complete` column to `False`.\n",
    "    When the work is complete we write an additional row setting this column to `True`. Alternate\n",
    "    workflows may only include writing to the table when work is complete. It is sometimes favorable\n",
    "    to keep track of when jobs start and finish so you can produce additional metrics around\n",
    "    when the jobs ran and how long they took.\n",
    "    \"\"\"\n",
    "    client.append_to_table(\n",
    "        STATUS_TABLE_NAME,\n",
    "        rerun_partition_id=partition_list,\n",
    "        is_complete=[False] * len(partition_list),\n",
    "        update_time=[datetime.now()] * len(partition_list),\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        dataset.dataframe_query_view(index=\"time_1\", contents=\"/**\")\n",
    "        .filter_partition_id(*partition_list)\n",
    "        .df()\n",
    "    )\n",
    "\n",
    "    df = df.aggregate(\n",
    "        \"rerun_partition_id\",\n",
    "        [\n",
    "            F.min(col(\"log_time\")).alias(\"first_log_time\"),\n",
    "            F.max(col(\"log_time\")).alias(\"last_log_time\"),\n",
    "            F.first_value(\n",
    "                col(\"/obj1:Points3D:positions\")[0],\n",
    "                filter=col(\"/obj1:Points3D:positions\").is_not_null(),\n",
    "                order_by=col(\"time_1\"),\n",
    "            ).alias(\"first_position_obj1\"),\n",
    "            F.first_value(\n",
    "                col(\"/obj2:Points3D:positions\")[0],\n",
    "                filter=col(\"/obj2:Points3D:positions\").is_not_null(),\n",
    "                order_by=col(\"time_1\"),\n",
    "            ).alias(\"first_position_obj2\"),\n",
    "            F.first_value(\n",
    "                col(\"/obj3:Points3D:positions\")[0],\n",
    "                filter=col(\"/obj3:Points3D:positions\").is_not_null(),\n",
    "                order_by=col(\"time_1\"),\n",
    "            ).alias(\"first_position_obj3\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df.write_table(RESULTS_TABLE_NAME)\n",
    "\n",
    "    client.append_to_table(\n",
    "        STATUS_TABLE_NAME,\n",
    "        rerun_partition_id=partition_list,\n",
    "        is_complete=[True]\n",
    "        * len(\n",
    "            partition_list\n",
    "        ),  # Add the `True` value to prevent this from processing again\n",
    "        update_time=[datetime.now()] * len(partition_list),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98997083-0a57-49b3-b010-4095549dc647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is the main execution. If you wish to persist the directory you can remove\n",
    "# the `tempfile.TemporaryDirectory()` line and set `temp_path` to a directory on your machine.\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_path = Path(temp_dir)\n",
    "\n",
    "    client = CatalogClient(CATALOG_URL)\n",
    "    dataset = client.get_dataset(name=DATASET_NAME)\n",
    "\n",
    "    status_table = create_status_table(client, temp_path)\n",
    "    results_table = create_results_table(client, temp_path)\n",
    "\n",
    "    # TODO(tsaucer) replace with partition table query\n",
    "    partition_table = (\n",
    "        dataset.dataframe_query_view(index=\"time_1\", contents=\"/**\")\n",
    "        .df()\n",
    "        .select(\"rerun_partition_id\")\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    missing_partitions = None\n",
    "    while missing_partitions is None or len(missing_partitions) != 0:\n",
    "        missing_partitions = find_missing_partitions(partition_table, status_table)\n",
    "        print(\n",
    "            f\"{len(missing_partitions)} of {partition_table.count()} partitions have not processed.\"\n",
    "        )\n",
    "\n",
    "        if len(missing_partitions) > 0:\n",
    "            process_partitions(client, dataset, missing_partitions[0:3])\n",
    "\n",
    "    # Show the final results\n",
    "    display(results_table)\n",
    "\n",
    "    # Show the final status table\n",
    "    display(status_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b9c6a-a774-4fff-a746-3f342ec56d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
