syntax = "proto3";

package rerun.remote_store.v1alpha1;

import "rerun/v1alpha1/collection_ops.proto";
import "rerun/v1alpha1/common.proto";

service StorageNode {
  // Collection APIs

  rpc FindCollection(FindCollectionRequest) returns (FindCollectionResponse);
  rpc FindCollections(FindCollectionsRequest) returns (FindCollectionsResponse);

  // Data APIs

  rpc Query(QueryRequest) returns (stream DataframePart) {}

  rpc FetchRecording(FetchRecordingRequest) returns (stream rerun.common.v1alpha1.RerunChunk) {}

  // Index APIs

  rpc CreateIndex(CreateIndexRequest) returns (CreateIndexResponse) {}

  rpc ReIndex(ReIndexRequest) returns (ReIndexResponse) {}

  rpc GetChunks(GetChunksRequest) returns (stream rerun.common.v1alpha1.RerunChunk) {}

  rpc GetChunksRange(GetChunksRangeRequest) returns (stream rerun.common.v1alpha1.RerunChunk) {}

  // The response to `SearchIndex` a RecordBatch with 3 columns:
  // - 'resource_id' column with the id of the resource
  // - timepoint column with the values representing the points in time
  // where index query matches. What time points are matched depends on the type of
  // index that is queried. For example for vector search it might be timepoints where
  // top-K matches are found within *each* resource in the indexed entry. For inverted index
  // it might be timepoints where the query string is found in the indexed column
  // - 'data' column with the data that is returned for the matched timepoints
  rpc SearchIndex(SearchIndexRequest) returns (stream DataframePart) {}

  // Chunk manifest APIs

  rpc CreateManifests(CreateManifestsRequest) returns (CreateManifestsResponse) {}

  rpc ListManifests(ListManifestsRequest) returns (stream DataframePart) {}

  rpc QueryManifest(QueryManifestRequest) returns (stream DataframePart) {}

  // TODO(zehiko, cmc): DeleteManifest

  // Metadata APIs

  rpc QueryCatalog(QueryCatalogRequest) returns (stream DataframePart) {}

  rpc UpdateCatalog(UpdateCatalogRequest) returns (UpdateCatalogResponse) {}

  rpc GetRecordingSchema(GetRecordingSchemaRequest) returns (GetRecordingSchemaResponse) {}

  // Registration APIs

  // TODO(zehiko) support registering more than one recording at a time
  rpc RegisterRecording(RegisterRecordingRequest) returns (DataframePart) {}

  rpc UnregisterRecording(UnregisterRecordingRequest) returns (UnregisterRecordingResponse) {}

  rpc UnregisterAllRecordings(UnregisterAllRecordingsRequest) returns (UnregisterAllRecordingsResponse) {}
}

// ---------------- Common response message ------------------

// DataframePart is arrow IPC encoded RecordBatch
message DataframePart {
  // encoder version used to encode the data
  rerun.common.v1alpha1.EncoderVersion encoder_version = 1;

  // Data payload is Arrow IPC encoded RecordBatch
  bytes payload = 1000;
}

// ---------------- GetChunksRange ------------------

// GetChunksRange is a streaming API that allows to fetch chunks within a time range
message GetChunksRangeRequest {
  // which catalog entry do we want to fetch the chunks from
  CatalogEntry entry = 1;
  // recording id from which we're want to fetch the chunk
  rerun.common.v1alpha1.RecordingId recording_id = 2;
  // timeline for which we specify the time range
  rerun.common.v1alpha1.IndexColumnSelector time_index = 3;
  // time range for which we want to fetch the chunks
  rerun.common.v1alpha1.TimeRange time_range = 4;
}

// ---------------- GetChunk ---------------------

message GetChunksRequest {
  // which catalog entry do we want to fetch the chunks from
  CatalogEntry entry = 1;
  // recording id from which we're want to fetch the chunk ids
  rerun.common.v1alpha1.RecordingId recording_id = 2;
  // batch of chunk ids for which we want to stream back chunks
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;
}

// ---------------- CreateIndex ------------------

// used to define which column we want to index
message IndexColumn {
  // The path of the entity.
  rerun.common.v1alpha1.EntityPath entity_path = 1;
  // Optional name of the `Archetype` associated with this data.
  optional string archetype_name = 2;
  // Optional name of the field within `Archetype` associated with this data.
  optional string archetype_field_name = 3;
  // Semantic name associated with this data.
  string component_name = 4;
}

message CreateIndexRequest {
  // which catalog entry do we want to create index for
  CatalogEntry entry = 1;
  // what kind of index do we want to create and what are
  // its index specific properties
  IndexProperties properties = 2;
  // Component / column we want to index
  IndexColumn column = 3;
  // What is the filter index i.e. timeline for which we
  // will query the timepoints
  // TODO(zehiko) this might go away and we might just index
  // across all the timelines
  rerun.common.v1alpha1.IndexColumnSelector time_index = 4;
}

message ReIndexRequest {
  // which catalog entry do we want to reindex for
  CatalogEntry entry = 1;
  // which column do we want to reindex
  IndexColumn column = 2;
}

message IndexProperties {
  oneof props {
    InvertedIndex inverted = 1;
    VectorIvfPqIndex vector = 2;
    BTreeIndex btree = 3;
  }
}

message InvertedIndex {
  bool store_position = 1;
  string base_tokenizer = 2;
  // TODO(zehiko) add other properties as needed
}

message VectorIvfPqIndex {
  uint32 num_partitions = 1;
  uint32 num_sub_vectors = 2;
  VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
  VECTOR_DISTANCE_METRIC_UNSPECIFIED = 0;
  VECTOR_DISTANCE_METRIC_L2 = 1;
  VECTOR_DISTANCE_METRIC_COSINE = 2;
  VECTOR_DISTANCE_METRIC_DOT = 3;
  VECTOR_DISTANCE_METRIC_HAMMING = 4;
}

message BTreeIndex {
  // TODO(zehiko) add properties as needed
}

message CreateIndexResponse {
  uint64 indexed_rows = 1;
}

message ReIndexResponse {}

// ---------------- SearchIndex ------------------

message SearchIndexRequest {
  // The catalog entry that we want to search over
  // If not specified, the default collection is queried
  CatalogEntry entry = 1;
  // Index column that is queried
  IndexColumn column = 2;
  // Query data - type of data is index specific. Caller must ensure
  // to provide the right type. For vector search this should
  // be a vector of appropriate size, for inverted index this should be a string.
  // Query data is represented as a unit (single row) RecordBatch with 1 column.
  DataframePart query = 3;
  // Index type specific properties
  IndexQueryProperties properties = 4;
  // max number of rows to be returned
  optional uint32 limit = 5;
}

message IndexQueryProperties {
  // specific index query properties based on the index type
  oneof props {
    InvertedIndexQuery inverted = 1;
    VectorIndexQuery vector = 2;
    BTreeIndexQuery btree = 3;
  }
}

message InvertedIndexQuery {
  // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
  uint32 top_k = 2;
}

message BTreeIndexQuery {
  // TODO(zehiko) add properties as needed
}

message CatalogEntry {
  string name = 1;
}

// ---------------- CreateManifests ------------------

// TODO(zehiko, cmc): At some point, this will need to be fully async (i.e. caller gets assigned
// a unique request ID and polls it for completion), but:
// A) Let's wait until we have a real need for this.
// B) That's true of everything in the platform, so this needs to be properly generalized.

message CreateManifestsRequest {
  // Which catalog entry do we want to create manifests for?
  CatalogEntry entry = 1;
}

message CreateManifestsResponse {}

// ---------------- ListManifests ------------------

message ListManifestsRequest {
  // Which catalog entry do we want to list the manifests of?
  CatalogEntry entry = 1;

  // Generic parameters that will influence the behavior of the Lance scanner.
  //
  // TODO(zehiko, cmc): actually support those.
  ScanParameters scan_parameters = 500;
}

message ListManifestsResponse {
  rerun.common.v1alpha1.EncoderVersion encoder_version = 1;

  // The record batch of the response, encoded according to `encoder_version`.
  bytes payload = 2;
}

// ---------------- QueryManifest ------------------

// TODO(zehiko, cmc): Being able to specify only a collection ID rather than a resource ID could be
// super useful for cross-recording queries (resource_id becomes a column of the result).

// A manifest query will find all the relevant chunk IDs (and optionally a bunch of related metadata)
// for a given Rerun query (latest-at, range, etc).
//
// The result might contain duplicated chunk IDs, it is the responsibility of the caller to deduplicate
// them as needed.
message QueryManifestRequest {
  // Which catalog entry do we want to query the manifests of?
  CatalogEntry entry = 1;

  // What resource are we querying the manifest for?
  rerun.common.v1alpha1.RecordingId resource_id = 100;

  // What columns of the manifest are we interested in?
  ColumnProjection columns = 200;

  // If true, `columns` will contain the entire schema.
  bool columns_always_include_everything = 210;

  // If true, `columns` always includes `chunk_id`,
  bool columns_always_include_chunk_ids = 220;

  // If true, `columns` always includes `byte_offset` and `byte_size`.
  bool columns_always_include_byte_offsets = 230;

  // If true, `columns` always includes `entity_path`.
  bool columns_always_include_entity_paths = 240;

  // If true, `columns` always includes all static component-level indexes.
  bool columns_always_include_static_indexes = 250;

  // If true, `columns` always includes all temporal chunk-level indexes.
  bool columns_always_include_global_indexes = 260;

  // If true, `columns` always includes all component-level indexes.
  bool columns_always_include_component_indexes = 270;

  // If specified, will perform a latest-at query with the given parameters.
  //
  // Incompatible with `range`.
  QueryManifestLatestAtRelevantChunks latest_at = 300;

  // If specified, will perform a range query with the given parameters.
  //
  // Incompatible with `latest_at`.
  QueryManifestRangeRelevantChunks range = 400;

  // Generic parameters that will influence the behavior of the Lance scanner.
  ScanParameters scan_parameters = 500;
}

message QueryManifestLatestAtRelevantChunks {
  // Which entity paths are we interested in?
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 10;

  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 20;

  // What index value are we looking for?
  int64 at = 30;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
  // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLine`, etc.
  repeated string fuzzy_descriptors = 40;
}

message QueryManifestRangeRelevantChunks {
  // Which entity paths are we interested in?
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 10;

  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 20;

  // What index range are we looking for?
  rerun.common.v1alpha1.TimeRange index_range = 30;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
  // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLine`, etc.
  repeated string fuzzy_descriptors = 40;
}

// Generic parameters that will influence the behavior of the Lance scanner.
//
// TODO(zehiko, cmc): This should be available for every endpoint that queries data in
// one way or another.
message ScanParameters {
  // An arbitrary filter expression that will be passed to the Lance scanner as-is.
  //
  // ```text
  // scanner.filter(filter)
  // ```
  string filter = 100;

  // An arbitrary offset that will be passed to the Lance scanner as-is.
  //
  // ```text
  // scanner.limit(_, limit_offset)
  // ```
  int64 limit_offset = 200;

  // An arbitrary limit that will be passed to the Lance scanner as-is.
  //
  // ```text
  // scanner.limit(limit_len, _)
  // ```
  int64 limit_len = 201;

  // An arbitrary order clause that will be passed to the Lance scanner as-is.
  //
  // ```text
  // scanner.order_by(â€¦)
  // ```
  ScanParametersOrderClause order_by = 300;

  // If set, the output of `scanner.explain_plan` will be dumped to the server's log.
  bool explain_plan = 400;

  // If set, the final `scanner.filter` will be dumped to the server's log.
  bool explain_filter = 401;
}

message ScanParametersOrderClause {
  bool ascending = 10;
  bool nulls_first = 20;
  string column_name = 30;
}

// ---------------- GetRecordingSchema ------------------

message GetRecordingSchemaRequest {
  // which catalog entry do we want to fetch the schema from
  CatalogEntry entry = 1;
  // recording id from which we're want to fetch the schema
  rerun.common.v1alpha1.RecordingId recording_id = 2;
}

message GetRecordingSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 2;
}

// ---------------- RegisterRecording ------------------

message RegisterRecordingRequest {
  // to which catalog entry do we want to register the recording
  CatalogEntry entry = 1;
  // human readable description of the recording
  string description = 2;
  // recording storage url (e.g. s3://bucket/file or file:///path/to/file)
  string storage_url = 3;
  // type of recording
  RecordingType typ = 4;
  // (optional) any additional metadata that should be associated with the recording
  // You can associate any arbtrirary number of columns with a specific recording
  DataframePart metadata = 5;
}

// ---------------- Unregister from catalog ------------------

message UnregisterRecordingRequest {
  // which catalog entry do we want to unregister the recording from
  CatalogEntry entry = 1;
  // unique identifier of the recording
  rerun.common.v1alpha1.RecordingId recording_id = 2;
}
message UnregisterRecordingResponse {}

message UnregisterAllRecordingsRequest {
  // which catalog entry do we want to unregister all recordings from
  CatalogEntry entry = 1;
}
message UnregisterAllRecordingsResponse {}

// ---------------- UpdateCatalog  -----------------

message UpdateCatalogRequest {
  // which catalog entry do we want to update
  CatalogEntry entry = 1;
  // Properties that we want to update
  DataframePart metadata = 2;
}

message UpdateCatalogResponse {}

// ---------------- Query -----------------

message QueryRequest {
  // which catalog entry do we want to query
  CatalogEntry entry = 1;
  // unique identifier of the recording
  rerun.common.v1alpha1.RecordingId recording_id = 2;
  // query to execute
  rerun.common.v1alpha1.Query query = 3;
}

// ----------------- QueryCatalog -----------------

message QueryCatalogRequest {
  // which catalog entry do we want to query
  CatalogEntry entry = 1;
  // Column projection - define which columns should be returned.
  // Providing it is optional, if not provided, all columns should be returned
  ColumnProjection column_projection = 2;
  // Filter specific recordings that match the criteria (selection)
  CatalogFilter filter = 3;
}

message ColumnProjection {
  repeated string columns = 1;
}

message CatalogFilter {
  // Filtering is very simple right now, we can only select
  // recordings by their ids.
  repeated rerun.common.v1alpha1.RecordingId recording_ids = 1;
}

message QueryCatalogResponse {
  rerun.common.v1alpha1.EncoderVersion encoder_version = 1;
  // raw bytes are TransportChunks (i.e. RecordBatches) encoded with the relevant codec
  bytes payload = 2;
}

enum RecordingType {
  RECORDING_TYPE_UNSPECIFIED = 0;
  RECORDING_TYPE_RRD = 1;
}

// ----------------- FetchRecording -----------------

message FetchRecordingRequest {
  // which catalog entry do we want to fetch the recording from
  CatalogEntry entry = 1;
  // recording id from which we're want to fetch the recording
  rerun.common.v1alpha1.RecordingId recording_id = 2;
}

// TODO(jleibs): Eventually this becomes either query-mediated in some way, but for now
// it's useful to be able to just get back the whole RRD somehow.
message FetchRecordingResponse {
  // TODO(zehiko) we need to expand this to become something like 'encoder options'
  // as we will need to specify additional options like compression, including schema
  // in payload, etc.
  rerun.common.v1alpha1.EncoderVersion encoder_version = 1;
  // payload is raw bytes that the relevant codec can interpret
  bytes payload = 2;
}

// ----------------- Error handling -----------------

// Application level error - used as `details` in the `google.rpc.Status` message
message RemoteStoreError {
  // error code
  ErrorCode code = 1;
  // unique identifier associated with the request (e.g. recording id, recording storage url)
  string id = 2;
  // human readable details about the error
  string message = 3;
}

// Error codes for application level errors
enum ErrorCode {
  // unused
  ERROR_CODE_UNSPECIFIED = 0;

  // object store access error
  ERROR_CODE_OBJECT_STORE_ERROR = 1;

  // metadata database access error
  ERROR_CODE_METADATA_DB_ERROR = 2;

  // Encoding / decoding error
  ERROR_CODE_CODEC_ERROR = 3;
}
