syntax = "proto3";

package rerun.manifest_registry.v1alpha1;

import "rerun/v1alpha1/common.proto";

service ManifestRegistryService {
  // --- Write data ---

  // Register new partitions with the Dataset
  rpc RegisterWithDataset(RegisterWithDatasetRequest) returns (RegisterWithDatasetResponse) {}

  rpc RegisterWithDatasetAsync(RegisterWithDatasetAsyncRequest) returns (RegisterWithDatasetAsyncResponse) {}

  // Unimplemented.
  rpc WriteChunks(stream WriteChunksRequest) returns (stream WriteChunksResponse) {}

  // --- Query schemas ---

  // Returns the schema of the partition table (i.e. the dataset manifest) itself, *not* the underlying dataset.
  //
  // * To inspect the data of the partition table, use `ScanPartitionTable`.
  // * To retrieve the schema of the underlying dataset, use `GetDatasetSchema` instead.
  rpc GetPartitionTableSchema(GetPartitionTableSchemaRequest) returns (GetPartitionTableSchemaResponse) {}

  // Inspect the contents of the partition table (i.e. the dataset manifest).
  //
  // The returned data will follow the schema specified by `GetPartitionTableSchema`.
  rpc ScanPartitionTable(ScanPartitionTableRequest) returns (stream ScanPartitionTableResponse) {}

  // Returns the schema of the dataset.
  //
  // This is the union of all the schemas from all the underlying partitions. It will contain all the indexes,
  // entities and components present in the dataset.
  rpc GetDatasetSchema(GetDatasetSchemaRequest) returns (GetDatasetSchemaResponse) {}

  // --- Indexing ---

  // Creates a custom index for a specific column (vector search, full-text search, etc).
  //
  // Index can be created for all or specific partitions. Creating an index will create a new
  // index-specific chunk manifest for the Dataset.
  // Chunk manifest contains information about individual chunk rows for all chunks containing
  // relevant index data.
  rpc CreateIndex(CreateIndexRequest) returns (CreateIndexResponse) {}

  // Recreate an index with the same configuration but (potentially) new data.
  rpc ReIndex(ReIndexRequest) returns (ReIndexResponse) {}

  // --- Queries ---

  // Search a previously created index.
  //
  // Do a full text, vector or scalar search. Currently only an Indexed search is supported, user must first
  // call `CreateIndex` for the relevant column.
  //
  // The response is a RecordBatch with 4 columns:
  // - 'partition_id': which partition the data is from
  // - 'timepoint':  represents the points in time where index query matches.
  //     What time points are matched depends on the type of index that is queried.
  //     For example: for vector search it might be timepoints where top-K matches are found within *each* partition in
  //     the indexed entry.
  //     For inverted index it might be timepoints where the query string is found in the indexed column
  // - instance column: if index column contains a batch of values (for example a list of embeddings), then each
  //     instance of the batch is a separate row in the resulting RecordBatch
  // - instance_id: this is a simple element index in the batch array. For example if indexed column is a list of
  //     embeddings \[a,b,c\] (where each embedding is of same length) then 'instance_id' of embedding 'a' is 0,
  //     'instance_id' of 'b' is 1, etc.
  //
  // TODO(zehiko) add support for "brute force" search.
  rpc SearchDataset(SearchDatasetRequest) returns (stream SearchDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the matching chunk IDs.
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch the actual chunks themselves, see `GetChunks`.
  //
  // Passing chunk IDs to this method effectively acts as a IF_EXIST filter.
  rpc QueryDataset(QueryDatasetRequest) returns (stream QueryDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the underlying chunks.
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch only the actual chunk IDs rather than the chunks themselves, see `QueryDataset`.
  rpc GetChunks(GetChunksRequest) returns (stream GetChunksResponse) {}

  // --- Developer/Debug APIs ---
  // Unstable APIs that are not exposed by the Frontend.

  // Fetch an entire partition from the server, without any pre- or post-processing.
  //
  // This is a temporary hack while we get everything up and running.
  rpc FetchPartition(FetchPartitionRequest) returns (stream FetchPartitionResponse) {}

  // Retrieves the chunk manifest for a specific index.
  rpc FetchChunkManifest(FetchChunkManifestRequest) returns (stream FetchChunkManifestResponse) {}

  // Create manifests for all partitions in the Dataset. Partition manifest contains information about
  // the chunks in the partitions.
  //
  // This is normally automatically done as part of the registration process.
  rpc CreatePartitionManifests(CreatePartitionManifestsRequest) returns (CreatePartitionManifestsResponse) {}

  // Fetch the internal state of a Partition Manifest.
  rpc FetchPartitionManifest(FetchPartitionManifestRequest) returns (stream FetchPartitionManifestResponse) {}
}

// --- Write data ---

message DataSource {
  // Where is the data for this data source stored (e.g. s3://bucket/file or file:///path/to/file)?
  optional string storage_url = 1;

  // What kind of data is it (e.g. rrd, mcap, Lance, etc)?
  DataSourceKind typ = 2;
}

enum DataSourceKind {
  DATA_SOURCE_KIND_UNSPECIFIED = 0;
  DATA_SOURCE_KIND_RRD = 1;
}

message RegisterWithDatasetRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  repeated DataSource data_sources = 2;
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

message RegisterWithDatasetResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message RegisterWithDatasetAsyncRequest {
  // today this just wraps the sync request. This at least avoids
  // repetition and the two messages going out of sync.
  RegisterWithDatasetRequest request = 1;
}

message RegisterWithDatasetAsyncResponse {
  rerun.common.v1alpha1.TaskId id = 1;
}

message WriteChunksRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
}

message WriteChunksResponse {}

// --- Query schemas ---

message GetPartitionTableSchemaRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
}

message GetPartitionTableSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

message ScanPartitionTableRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message ScanPartitionTableResponse {
  // Partitions metadata as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message GetDatasetSchemaRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
}

message GetDatasetSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

// --- Indexing ---

message CreateIndexRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // List of specific partitions that will be indexed (all if left empty).
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  IndexConfig config = 3;

  // Specify behavior when index for a partition was already created.
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 4;
}

message IndexConfig {
  // what kind of index do we want to create and what are its index specific properties.
  IndexProperties properties = 1;

  // Component / column we want to index.
  IndexColumn column = 2;

  // What is the filter index i.e. timeline for which we will query the timepoints.
  //
  // TODO(zehiko) this might go away and we might just index across all the timelines
  rerun.common.v1alpha1.IndexColumnSelector time_index = 3;
}

// used to define which column we want to index
message IndexColumn {
  // The path of the entity.
  rerun.common.v1alpha1.EntityPath entity_path = 1;

  // Component details
  rerun.common.v1alpha1.ComponentDescriptor component = 2;
}

message IndexProperties {
  oneof props {
    InvertedIndex inverted = 1;
    VectorIvfPqIndex vector = 2;
    BTreeIndex btree = 3;
  }
}

message InvertedIndex {
  optional bool store_position = 1;
  optional string base_tokenizer = 2;
  // TODO(zehiko) add other properties as needed
}

message VectorIvfPqIndex {
  optional uint32 num_partitions = 1;
  optional uint32 num_sub_vectors = 2;
  VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
  VECTOR_DISTANCE_METRIC_UNSPECIFIED = 0;
  VECTOR_DISTANCE_METRIC_L2 = 1;
  VECTOR_DISTANCE_METRIC_COSINE = 2;
  VECTOR_DISTANCE_METRIC_DOT = 3;
  VECTOR_DISTANCE_METRIC_HAMMING = 4;
}

message BTreeIndex {
  // TODO(zehiko) add properties as needed
}

message CreateIndexResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message ReIndexRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
}

message ReIndexResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// --- Queries ---

message SearchDatasetRequest {
  // Dataset for which we want to search index
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Index column that is queried
  IndexColumn column = 2;

  // Query data - type of data is index specific. Caller must ensure
  // to provide the right type. For vector search this should
  // be a vector of appropriate size, for inverted index this should be a string.
  // Query data is represented as a unit (single row) RecordBatch with 1 column.
  rerun.common.v1alpha1.DataframePart query = 3;

  // Index type specific properties
  IndexQueryProperties properties = 4;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;
}

message SearchDatasetResponse {
  // Chunks as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message IndexQueryProperties {
  // specific index query properties based on the index type
  oneof props {
    InvertedIndexQuery inverted = 1;
    VectorIndexQuery vector = 2;
    BTreeIndexQuery btree = 3;
  }
}

message InvertedIndexQuery {
  // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
  optional uint32 top_k = 1;
}

message BTreeIndexQuery {
  // TODO(zehiko) add properties as needed
}

message QueryDatasetRequest {
  // Dataset client wants to query
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Client can specify what partitions are queried. If left unspecified (empty list),
  // all partitions will be queried.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify specific chunk ids to include. If left unspecified (empty list),
  // all chunks that match other query parameters will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // Generic parameters that will influence the behavior of the Lance scanner.
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;

  // A chunk-level latest-at or range query, or both.
  //
  // This query is AND'd together with the `partition_ids` and `chunk_ids` filters above.
  Query query = 6;
}

message QueryDatasetResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message Query {
  // If specified, will perform a latest-at query with the given parameters.
  //
  // You can combine this with a `QueryRange` in order to gather all the relevant chunks for
  // a full-fledged dataframe query (i.e. they get OR'd together).
  QueryLatestAt latest_at = 1;

  // If specified, will perform a range query with the given parameters.
  //
  // You can combine this with a `QueryLatestAt` in order to gather all the relevant chunks for
  // a full-fledged dataframe query (i.e. they get OR'd together).
  QueryRange range = 2;

  // If true, `columns` will contain the entire schema.
  bool columns_always_include_everything = 3;

  // If true, `columns` always includes `chunk_id`,
  bool columns_always_include_chunk_ids = 4;

  // If true, `columns` always includes `byte_offset` and `byte_size`.
  bool columns_always_include_byte_offsets = 5;

  // If true, `columns` always includes `entity_path`.
  bool columns_always_include_entity_paths = 6;

  // If true, `columns` always includes all static component-level indexes.
  bool columns_always_include_static_indexes = 7;

  // If true, `columns` always includes all temporal chunk-level indexes.
  bool columns_always_include_global_indexes = 8;

  // If true, `columns` always includes all component-level indexes.
  bool columns_always_include_component_indexes = 9;
}

// A chunk-level latest-at query, aka `LatestAtRelevantChunks`.
//
// This has the exact same semantics as the query of the same name on our `ChunkStore`.
message QueryLatestAt {
  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 1;

  // What index value are we looking for?
  optional int64 at = 2;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
  // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLine`, etc.
  repeated string fuzzy_descriptors = 3;
  // TODO(cmc): I shall bring that back into a more structured form later.
  // repeated rerun.common.v1alpha1.ComponentDescriptor fuzzy_descriptors = 3;
}

/// A chunk-level range query, aka `RangeRelevantChunks`.
//
// This has the exact same semantics as the query of the same name on our `ChunkStore`.
message QueryRange {
  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 1;

  // What index range are we looking for?
  rerun.common.v1alpha1.TimeRange index_range = 2;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
  // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLine`, etc.
  repeated string fuzzy_descriptors = 3;
  // TODO(cmc): I shall bring that back into a more structured form later.
  // repeated rerun.common.v1alpha1.ComponentDescriptor fuzzy_descriptors = 3;
}

message GetChunksRequest {
  // Dataset for which we want to get chunks
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Client can specify from which partitions to get chunks. If left unspecified (empty list),
  // data from all partition (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify chunk ids to include. If left unspecified (empty list),
  // all chunks (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // A chunk-level latest-at or range query, or both.
  //
  // This query is AND'd together with the `partition_ids` and `chunk_ids` filters above.
  Query query = 5;
}

message GetChunksResponse {
  rerun.common.v1alpha1.RerunChunk chunk = 1;
}

// --- Developer/Debug APIs ---
// Unstable APIs that are not exposed by the Frontend.

message CreatePartitionManifestsRequest {
  // Dataset for which we want to create manifests
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Create manifest for specific partitions. All will be
  // created if left unspecified (empty list)
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Define what happens if create is called multiple times for the same
  // Dataset / partitions
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

message CreatePartitionManifestsResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message FetchPartitionManifestRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  rerun.common.v1alpha1.PartitionId id = 2;
  rerun.common.v1alpha1.ScanParameters scan_parameters = 3;
}

// TODO(cmc): this should have response extensions too.
message FetchPartitionManifestResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message FetchPartitionRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  rerun.common.v1alpha1.PartitionId partition_id = 2;
}

message FetchPartitionResponse {
  // Chunks as arrow RecordBatch
  rerun.common.v1alpha1.RerunChunk chunk = 1;
}

message FetchChunkManifestRequest {
  // Dataset for which we want to fetch chunk manifest
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Chunk manifest is index specific
  IndexColumn column = 2;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 3;
}

message FetchChunkManifestResponse {
  // Chunk manifest as arrow RecordBatches
  rerun.common.v1alpha1.DataframePart data = 1;
}

// ----------------- Error handling -----------------

// Application level error - used as `details` in the `google.rpc.Status` message
message Error {
  // error code
  ErrorCode code = 1;
  // unique identifier associated with the request (e.g. recording id, recording storage url)
  string id = 2;
  // human readable details about the error
  string message = 3;
}

// Error codes for application level errors
enum ErrorCode {
  // unused
  ERROR_CODE_UNSPECIFIED = 0;

  // object store access error
  ERROR_CODE_OBJECT_STORE_ERROR = 1;

  // metadata database access error
  ERROR_CODE_METADATA_DB_ERROR = 2;

  // Encoding / decoding error
  ERROR_CODE_CODEC_ERROR = 3;
}
