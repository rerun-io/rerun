syntax = "proto3";

package rerun.manifest_registry.v1alpha1;

import "rerun/v1alpha1/common.proto";

service ManifestRegistryService {
  // Register new partitions with the Dataset
  rpc RegisterPartitions(RegisterPartitionsRequest) returns (RegisterPartitionsResponse) {}

  // Unregister partitions from the Dataset
  rpc UnregisterPartitions(UnregisterPartitionsRequest) returns (UnregisterPartitionsResponse) {}

  // List partitions in the Dataset
  rpc ListPartitions(ListPartitionsRequest) returns (stream ListPartitionsResponse) {}

  // Create manifests for all partitions in the Dataset. Partition manifest contains information about
  // the chunks in the partitions.
  rpc CreatePartitionManifests(CreatePartitionManifestsRequest) returns (CreatePartitionManifestsResponse) {}

  // Query Dataset and return a dataframe containing relevant chunk IDs
  rpc QueryDataset(QueryDatasetRequest) returns (stream QueryDatasetResponse) {}

  // Get all chunks for a single partition
  rpc FetchPartition(FetchPartitionRequest) returns (stream FetchPartitionResponse) {}

  // Get chunks based on a specific query. This call is similar to `QueryDataset` but
  // instead of returning a dataframe with relevant chunk IDs, it returns the actual chunks.
  rpc GetChunks(GetChunksRequest) returns (stream GetChunksResponse) {}

  // Create index for partitions in the Dataset. Index can be created for all or specific
  // partitions. Creating an index will create a new index-specific chunk manifest for the Dataset.
  // Chunk manifest contains information about individual chunk rows for all chunks containing relevant
  // index data.
  rpc CreatePartitionIndexes(CreatePartitionIndexesRequest) returns (CreatePartitionIndexesResponse) {}

  // Fetch chunk manifest for a specific index
  rpc FetchChunkManifest(FetchChunkManifestRequest) returns (stream FetchChunkManifestResponse) {}

  // Do a full text, vector or scalar search. Currently only an Indexed search
  // is supported, user must first call `CreatePartitionIndexes` for the relevant column.
  // TODO(zehiko) add support for "brute force" search.
  // The response is a RecordBatch with 4 columns:
  // - 'partition_id' - which partition the data is from
  // - 'timepoint' -  represents the points in time where index query matches.
  // What time points are matched depends on the type of index that is queried.
  // For example: for vector search it might be timepoints where top-K matches
  // are found within *each* partition in the indexed entry. For inverted index
  // it might be timepoints where the query string is found in the indexed column
  // - instance column - if index column contains a batch of values (for example a
  // list of embeddings), then each instance of the batch is a separate row in the
  // resulting RecordBatch
  // - 'instance_id' - this is a simple element index in the batch array. For example
  // if indexed column is a list of embeddings \[a,b,c\] (where each embedding is of same length)
  // then 'instance_id' of embedding 'a' is 0, 'instance_id' of 'b' is 1, etc.
  rpc SearchDataset(SearchDatasetRequest) returns (stream SearchDatasetResponse) {}
}

message Partition {
  // human readable description of the partition. Can be an empty string.
  optional string description = 1;

  // partition storage url (e.g. s3://bucket/file or file:///path/to/file)
  optional string storage_url = 2;

  // type of partition (rrd, mcap, Lance, etc)
  PartitionType typ = 3;
}

enum PartitionType {
  PARTITION_TYPE_UNSPECIFIED = 0;
  PARTITION_TYPE_RRD = 1;
}

message RegisterPartitionsRequest {
  // Dataset entry
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Partitions to add
  repeated Partition partitions = 2;

  // what to do if partition is already registered
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

message RegisterPartitionsResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message UnregisterPartitionsRequest {
  // Dataset from which we want to remove partitions
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Partitions to remove
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // What to do if partition is not found
  rerun.common.v1alpha1.IfMissingBehavior on_unknown_partition = 3;
}

message UnregisterPartitionsResponse {
  // returns a list of partitions that were actually removed
  rerun.common.v1alpha1.DataframePart data = 1;
}

message ListPartitionsRequest {
  // Dataset for which we want to list partitions
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message ListPartitionsResponse {
  // Partitions metadata as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message CreatePartitionManifestsRequest {
  // Dataset for which we want to create manifests
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Create manifest for specific partitions. All will be
  // created if left unspecified (empty list)
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Define what happens if create is called multiple times for the same
  // Dataset / partitions
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

message CreatePartitionManifestsResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message QueryDatasetRequest {
  // Dataset client wants to query
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Client can specify what partitions are queried. If left unspecified (empty list),
  // all partitions will be queried.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify specific chunk ids to include. If left unspecified (empty list),
  // all chunks that match other query parameters will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Generic parameters that will influence the behavior of the Lance scanner.
  rerun.common.v1alpha1.ScanParameters scan_parameters = 4;

  Query query = 5;
}

message QueryDatasetResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message QueryManifestLatestAtRelevantChunks {
  // Which entity paths are we interested in? Empty list means all entity paths
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 1;

  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 2;

  // What index value are we looking for?
  optional int64 at = 3;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
  // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLine`, etc.
  repeated ComponentDescriptor fuzzy_descriptors = 4;
}

message QueryManifestRangeRelevantChunks {
  // Which entity paths are we interested in?
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 1;

  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 2;

  // What index range are we looking for?
  rerun.common.v1alpha1.TimeRange index_range = 3;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
  // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLine`, etc.
  repeated ComponentDescriptor fuzzy_descriptors = 4;
}

message FetchPartitionRequest {
  // Partition's Dataset
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Partition for which we want to get chunks
  rerun.common.v1alpha1.PartitionId partition_id = 2;
}

message FetchPartitionResponse {
  // Chunks as arrow RecordBatch
  rerun.common.v1alpha1.RerunChunk chunk = 1;
}

message GetChunksRequest {
  // Dataset for which we want to get chunks
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Client can specify from which partitions to get chunks. If left unspecified (empty list),
  // data from all partition (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify chunk ids to include. If left unspecified (empty list),
  // all chunks (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Query details
  Query query = 4;
}

message GetChunksResponse {
  rerun.common.v1alpha1.RerunChunk chunk = 1;
}

message CreatePartitionIndexesRequest {
  // Dataset for which we want to create index
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // List of specific partitions that will be indexes.
  // All partitions will be indexed if left unspecified (empty list)
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // what kind of index do we want to create and what are
  // its index specific properties
  IndexProperties properties = 3;

  // Component / column we want to index
  IndexColumn column = 4;

  // What is the filter index i.e. timeline for which we
  // will query the timepoints
  // TODO(zehiko) this might go away and we might just index
  // across all the timelines
  rerun.common.v1alpha1.IndexColumnSelector time_index = 5;

  // Specify behavior when index for a partition was already created
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 6;
}

// used to define which column we want to index
message IndexColumn {
  // The path of the entity.
  rerun.common.v1alpha1.EntityPath entity_path = 1;

  // Component details
  ComponentDescriptor component = 2;
}

message IndexProperties {
  oneof props {
    InvertedIndex inverted = 1;
    VectorIvfPqIndex vector = 2;
    BTreeIndex btree = 3;
  }
}

message InvertedIndex {
  optional bool store_position = 1;
  optional string base_tokenizer = 2;
  // TODO(zehiko) add other properties as needed
}

message VectorIvfPqIndex {
  optional uint32 num_partitions = 1;
  optional uint32 num_sub_vectors = 2;
  VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
  VECTOR_DISTANCE_METRIC_UNSPECIFIED = 0;
  VECTOR_DISTANCE_METRIC_L2 = 1;
  VECTOR_DISTANCE_METRIC_COSINE = 2;
  VECTOR_DISTANCE_METRIC_DOT = 3;
  VECTOR_DISTANCE_METRIC_HAMMING = 4;
}

message BTreeIndex {
  // TODO(zehiko) add properties as needed
}

message CreatePartitionIndexesResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message FetchChunkManifestRequest {
  // Dataset for which we want to fetch chunk manifest
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Chunk manifest is index specific
  IndexColumn column = 2;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 3;
}

message FetchChunkManifestResponse {
  // Chunk manifest as arrow RecordBatches
  rerun.common.v1alpha1.DataframePart data = 1;
}

message ListPartitionIndexesRequest {
  // Dataset for which we want to list indexes
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message ListPartitionIndexesResponse {
  // Indexes metadata as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message SearchDatasetRequest {
  // Dataset for which we want to search index
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Index column that is queried
  IndexColumn column = 2;

  // Query data - type of data is index specific. Caller must ensure
  // to provide the right type. For vector search this should
  // be a vector of appropriate size, for inverted index this should be a string.
  // Query data is represented as a unit (single row) RecordBatch with 1 column.
  rerun.common.v1alpha1.DataframePart query = 3;

  // Index type specific properties
  IndexQueryProperties properties = 4;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;
}

message SearchDatasetResponse {
  // Chunks as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message IndexQueryProperties {
  // specific index query properties based on the index type
  oneof props {
    InvertedIndexQuery inverted = 1;
    VectorIndexQuery vector = 2;
    BTreeIndexQuery btree = 3;
  }
}

message InvertedIndexQuery {
  // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
  optional uint32 top_k = 1;
}

message BTreeIndexQuery {
  // TODO(zehiko) add properties as needed
}

message ComponentDescriptor {
  // Optional name of the `Archetype` associated with this data.
  optional string archetype_name = 1;

  // Optional name of the field within `Archetype` associated with this data.
  optional string archetype_field_name = 2;

  // Semantic name associated with this data.
  optional string component_name = 3;
}

message Query {
  // If true, `columns` will contain the entire schema.
  bool columns_always_include_everything = 4;

  // If true, `columns` always includes `chunk_id`,
  bool columns_always_include_chunk_ids = 5;

  // If true, `columns` always includes `byte_offset` and `byte_size`.
  bool columns_always_include_byte_offsets = 6;

  // If true, `columns` always includes `entity_path`.
  bool columns_always_include_entity_paths = 7;

  // If true, `columns` always includes all static component-level indexes.
  bool columns_always_include_static_indexes = 8;

  // If true, `columns` always includes all temporal chunk-level indexes.
  bool columns_always_include_global_indexes = 9;

  // If true, `columns` always includes all component-level indexes.
  bool columns_always_include_component_indexes = 10;

  // If specified, will perform a latest-at query with the given parameters.
  QueryManifestLatestAtRelevantChunks latest_at = 11;

  // If specified, will perform a range query with the given parameters.
  QueryManifestRangeRelevantChunks range = 12;
}
