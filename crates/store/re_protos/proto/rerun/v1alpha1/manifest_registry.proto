syntax = "proto3";

package rerun.manifest_registry.v1alpha1;

import "rerun/v1alpha1/common.proto";
import "rerun/v1alpha1/log_msg.proto";

service ManifestRegistryService {
  // --- Write data ---

  // Register new partitions with the Dataset (asynchronously)
  rpc RegisterWithDataset(RegisterWithDatasetRequest) returns (RegisterWithDatasetResponse) {}

  // Register new partitions with the Dataset (blocking)
  rpc RegisterWithDatasetBlocking(RegisterWithDatasetBlockingRequest) returns (RegisterWithDatasetBlockingResponse) {}

  // Write chunks to one or more partitions.
  //
  // The partition ID for each individual chunk is extracted from their metadata (`rerun.partition_id`).
  //
  // The destination dataset URL must be provided in the `x-rerun-dataset-url` header.
  rpc WriteChunks(stream WriteChunksRequest) returns (WriteChunksResponse) {}

  // --- Query schemas ---

  // Returns the schema of the partition table (i.e. the dataset manifest) itself, *not* the underlying dataset.
  //
  // * To inspect the data of the partition table, use `ScanPartitionTable`.
  // * To retrieve the schema of the underlying dataset, use `GetDatasetSchema` instead.
  rpc GetPartitionTableSchema(GetPartitionTableSchemaRequest) returns (GetPartitionTableSchemaResponse) {}

  // Inspect the contents of the partition table (i.e. the dataset manifest).
  //
  // The returned data will follow the schema specified by `GetPartitionTableSchema`.
  rpc ScanPartitionTable(ScanPartitionTableRequest) returns (stream ScanPartitionTableResponse) {}

  // Returns the schema of the dataset.
  //
  // This is the union of all the schemas from all the underlying partitions. It will contain all the indexes,
  // entities and components present in the dataset.
  rpc GetDatasetSchema(GetDatasetSchemaRequest) returns (GetDatasetSchemaResponse) {}

  // --- Indexing ---

  // Creates a custom index for a specific column (vector search, full-text search, etc).
  //
  // Index can be created for all or specific partitions. Creating an index will create a new
  // index-specific chunk manifest for the Dataset.
  // Chunk manifest contains information about individual chunk rows for all chunks containing
  // relevant index data.
  rpc CreateIndex(CreateIndexRequest) returns (CreateIndexResponse) {}

  // Recreate an index with the same configuration but (potentially) new data.
  rpc ReIndex(ReIndexRequest) returns (ReIndexResponse) {}

  // --- Queries ---

  // Search a previously created index.
  //
  // Do a full text, vector or scalar search. Currently only an Indexed search is supported, user must first
  // call `CreateIndex` for the relevant column.
  //
  // The response is a RecordBatch with 4 columns:
  // - 'partition_id': which partition the data is from
  // - 'timepoint':  represents the points in time where index query matches.
  //     What time points are matched depends on the type of index that is queried.
  //     For example: for vector search it might be timepoints where top-K matches are found within *each* partition in
  //     the indexed entry.
  //     For inverted index it might be timepoints where the query string is found in the indexed column
  // - instance column: if index column contains a batch of values (for example a list of embeddings), then each
  //     instance of the batch is a separate row in the resulting RecordBatch
  // - instance_id: this is a simple element index in the batch array. For example if indexed column is a list of
  //     embeddings \[a,b,c\] (where each embedding is of same length) then 'instance_id' of embedding 'a' is 0,
  //     'instance_id' of 'b' is 1, etc.
  //
  // TODO(zehiko) add support for "brute force" search.
  rpc SearchDataset(SearchDatasetRequest) returns (stream SearchDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the matching chunk IDs.
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch the actual chunks themselves, see `GetChunks`.
  //
  // Passing chunk IDs to this method effectively acts as a IF_EXIST filter.
  rpc QueryDataset(QueryDatasetRequest) returns (stream QueryDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the underlying chunks.
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch only the actual chunk IDs rather than the chunks themselves, see `QueryDataset`.
  rpc GetChunks(GetChunksRequest) returns (stream GetChunksResponse) {}

  // --- Developer/Debug APIs ---
  // Unstable APIs that are not exposed by the Frontend.

  // Retrieves the chunk manifest for a specific index.
  rpc FetchChunkManifest(FetchChunkManifestRequest) returns (stream FetchChunkManifestResponse) {}

  // Create manifests for all partitions in the Dataset. Partition manifest contains information about
  // the chunks in the partitions.
  //
  // This is normally automatically done as part of the registration process.
  rpc CreatePartitionManifests(CreatePartitionManifestsRequest) returns (CreatePartitionManifestsResponse) {}

  // Fetch the internal state of a Partition Manifest.
  rpc FetchPartitionManifest(FetchPartitionManifestRequest) returns (stream FetchPartitionManifestResponse) {}

  // Miscellaneous maintenance operations: scalar index creation, compaction, etc.
  rpc DoMaintenance(DoMaintenanceRequest) returns (DoMaintenanceResponse) {}
}

// --- Write data ---

message DataSource {
  // Where is the data for this data source stored (e.g. s3://bucket/file or file:///path/to/file)?
  optional string storage_url = 1;

  // What kind of data is it (e.g. rrd, mcap, Lance, etc)?
  DataSourceKind typ = 2;
}

enum DataSourceKind {
  DATA_SOURCE_KIND_UNSPECIFIED = 0;
  DATA_SOURCE_KIND_RRD = 1;
}

message RegisterWithDatasetRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  repeated DataSource data_sources = 2;
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

message RegisterWithDatasetResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// TODO(andrea): This is a copy of RegisterWithDatasetRequest.
// Eventually we _may_ get rid of the sync version; until then,
// we should make sure that the two objects are in sync.
message RegisterWithDatasetBlockingRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  repeated DataSource data_sources = 2;
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

message RegisterWithDatasetBlockingResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message WriteChunksRequest {
  rerun.common.v1alpha1.RerunChunk chunk = 1;
}

message WriteChunksResponse {}

// --- Query schemas ---

message GetPartitionTableSchemaRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
}

message GetPartitionTableSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

message ScanPartitionTableRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message ScanPartitionTableResponse {
  // Partitions metadata as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message GetDatasetSchemaRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
}

message GetDatasetSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

// --- Indexing ---

message CreateIndexRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // List of specific partitions that will be indexed (all if left empty).
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  IndexConfig config = 3;

  // Specify behavior when index for a partition was already created.
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 4;
}

message IndexConfig {
  // what kind of index do we want to create and what are its index specific properties.
  IndexProperties properties = 1;

  // Component / column we want to index.
  IndexColumn column = 2;

  // What is the filter index i.e. timeline for which we will query the timepoints.
  //
  // TODO(zehiko) this might go away and we might just index across all the timelines
  rerun.common.v1alpha1.IndexColumnSelector time_index = 3;
}

// used to define which column we want to index
message IndexColumn {
  // The path of the entity.
  rerun.common.v1alpha1.EntityPath entity_path = 1;

  // Component details
  rerun.common.v1alpha1.ComponentDescriptor component = 2;
}

message IndexProperties {
  oneof props {
    InvertedIndex inverted = 1;
    VectorIvfPqIndex vector = 2;
    BTreeIndex btree = 3;
  }
}

message InvertedIndex {
  optional bool store_position = 1;
  optional string base_tokenizer = 2;
  // TODO(zehiko) add other properties as needed
}

message VectorIvfPqIndex {
  optional uint32 num_partitions = 1;
  optional uint32 num_sub_vectors = 2;
  VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
  VECTOR_DISTANCE_METRIC_UNSPECIFIED = 0;
  VECTOR_DISTANCE_METRIC_L2 = 1;
  VECTOR_DISTANCE_METRIC_COSINE = 2;
  VECTOR_DISTANCE_METRIC_DOT = 3;
  VECTOR_DISTANCE_METRIC_HAMMING = 4;
}

message BTreeIndex {
  // TODO(zehiko) add properties as needed
}

message CreateIndexResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message ReIndexRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
}

message ReIndexResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// --- Queries ---

message SearchDatasetRequest {
  // Dataset for which we want to search index
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Index column that is queried
  IndexColumn column = 2;

  // Query data - type of data is index specific. Caller must ensure
  // to provide the right type. For vector search this should
  // be a vector of appropriate size, for inverted index this should be a string.
  // Query data is represented as a unit (single row) RecordBatch with 1 column.
  rerun.common.v1alpha1.DataframePart query = 3;

  // Index type specific properties
  IndexQueryProperties properties = 4;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;
}

message SearchDatasetResponse {
  // Chunks as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message IndexQueryProperties {
  // specific index query properties based on the index type
  oneof props {
    InvertedIndexQuery inverted = 1;
    VectorIndexQuery vector = 2;
    BTreeIndexQuery btree = 3;
  }
}

message InvertedIndexQuery {
  // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
  optional uint32 top_k = 1;
}

message BTreeIndexQuery {
  // TODO(zehiko) add properties as needed
}

message QueryDatasetRequest {
  // Dataset client wants to query
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Client can specify what partitions are queried. If left unspecified (empty list),
  // all partitions will be queried.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify specific chunk ids to include. If left unspecified (empty list),
  // all chunks that match other query parameters will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty, and set `select_all_entity_paths`,
  // in order to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // If set, the query will cover all existing entity paths.
  //
  // `entity_paths` must be empty, otherwise an error will be raised.
  //
  // Truth table:
  // ```text
  // select_all_entity_paths | entity_paths   | result
  // ------------------------+----------------+--------
  // false                   | []             | valid query, empty results (no entity paths selected)
  // false                   | ['foo', 'bar'] | valid query, 'foo' & 'bar' selected
  // true                    | []             | valid query, all entity paths selected
  // true                    | ['foo', 'bar'] | invalid query, error
  // ```
  bool select_all_entity_paths = 7;

  // If set, static data will be excluded from the results.
  bool exclude_static_data = 8;

  // If set, temporal data will be excluded from the results.
  bool exclude_temporal_data = 9;

  // Generic parameters that will influence the behavior of the Lance scanner.
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;

  // A chunk-level latest-at or range query, or both.
  //
  // This query is AND'd together with the `partition_ids` and `chunk_ids` filters above.
  Query query = 6;
}

message QueryDatasetResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message Query {
  // If specified, will perform a latest-at query with the given parameters.
  //
  // You can combine this with a `QueryRange` in order to gather all the relevant chunks for
  // a full-fledged dataframe query (i.e. they get OR'd together).
  QueryLatestAt latest_at = 1;

  // If specified, will perform a range query with the given parameters.
  //
  // You can combine this with a `QueryLatestAt` in order to gather all the relevant chunks for
  // a full-fledged dataframe query (i.e. they get OR'd together).
  QueryRange range = 2;

  // If true, `columns` will contain the entire schema.
  bool columns_always_include_everything = 3;

  // If true, `columns` always includes `chunk_id`,
  bool columns_always_include_chunk_ids = 4;

  // If true, `columns` always includes `byte_offset` and `byte_size`.
  bool columns_always_include_byte_offsets = 5;

  // If true, `columns` always includes `entity_path`.
  bool columns_always_include_entity_paths = 6;

  // If true, `columns` always includes all static component-level indexes.
  bool columns_always_include_static_indexes = 7;

  // If true, `columns` always includes all temporal chunk-level indexes.
  bool columns_always_include_global_indexes = 8;

  // If true, `columns` always includes all component-level indexes.
  bool columns_always_include_component_indexes = 9;
}

// A chunk-level latest-at query, aka `LatestAtRelevantChunks`.
//
// This has the exact same semantics as the query of the same name on our `ChunkStore`.
message QueryLatestAt {
  // Which index column should we perform the query on? E.g. `log_time`.
  //
  // Leave this empty to query for static data.
  rerun.common.v1alpha1.IndexColumnSelector index = 1;

  // What index value are we looking for?
  //
  // Leave this empty to query for static data.
  optional int64 at = 2;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLines:StrokeWidth#width` index, all of the following
  // would match: `SeriesLines:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLines`, etc.
  repeated string fuzzy_descriptors = 3;
  // TODO(cmc): I shall bring that back into a more structured form later.
  // repeated rerun.common.v1alpha1.ComponentDescriptor fuzzy_descriptors = 3;
}

/// A chunk-level range query, aka `RangeRelevantChunks`.
//
// This has the exact same semantics as the query of the same name on our `ChunkStore`.
message QueryRange {
  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 1;

  // What index range are we looking for?
  rerun.common.v1alpha1.TimeRange index_range = 2;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLines:StrokeWidth#width` index, all of the following
  // would match: `SeriesLines:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
  // `SeriesLines`, etc.
  repeated string fuzzy_descriptors = 3;
  // TODO(cmc): I shall bring that back into a more structured form later.
  // repeated rerun.common.v1alpha1.ComponentDescriptor fuzzy_descriptors = 3;
}

message GetChunksRequest {
  // Dataset for which we want to get chunks
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Client can specify from which partitions to get chunks. If left unspecified (empty list),
  // data from all partition (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify chunk ids to include. If left unspecified (empty list),
  // all chunks (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty, and set `select_all_entity_paths`,
  // in order to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // If set, the query will cover all existing entity paths.
  //
  // `entity_paths` must be empty, otherwise an error will be raised.
  //
  // Truth table:
  // ```text
  // select_all_entity_paths | entity_paths   | result
  // ------------------------+----------------+--------
  // false                   | []             | valid query, empty results (no entity paths selected)
  // false                   | ['foo', 'bar'] | valid query, 'foo' & 'bar' selected
  // true                    | []             | valid query, all entity paths selected
  // true                    | ['foo', 'bar'] | invalid query, error
  // ```
  bool select_all_entity_paths = 6;

  // If set, static data will be excluded from the results.
  bool exclude_static_data = 7;

  // If set, temporal data will be excluded from the results.
  bool exclude_temporal_data = 8;

  // A chunk-level latest-at or range query, or both.
  //
  // This query is AND'd together with the `partition_ids` and `chunk_ids` filters above.
  Query query = 5;
}

message GetChunksResponse {
  // Every gRPC response, even within the confines of a stream, involves HTTP2 overhead, which isn't
  // cheap by any means, which is why we're returning a batch of `ArrowMsg` rather than a single one.
  repeated rerun.log_msg.v1alpha1.ArrowMsg chunks = 1;
}

// --- Developer/Debug APIs ---
// Unstable APIs that are not exposed by the Frontend.

message CreatePartitionManifestsRequest {
  // Dataset for which we want to create manifests
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Create manifest for specific partitions. All will be
  // created if left unspecified (empty list)
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // types of partitions and their storage location (same order
  // as partition ids above)
  repeated DataSource data_sources = 3;

  // Define what happens if create is called multiple times for the same
  // Dataset / partitions
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 4;
}

message CreatePartitionManifestsResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message FetchPartitionManifestRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  rerun.common.v1alpha1.PartitionId id = 2;
  rerun.common.v1alpha1.ScanParameters scan_parameters = 3;
}

// TODO(cmc): this should have response extensions too.
message FetchPartitionManifestResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message FetchChunkManifestRequest {
  // Dataset for which we want to fetch chunk manifest
  rerun.common.v1alpha1.DatasetHandle entry = 1;

  // Chunk manifest is index specific
  IndexColumn column = 2;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 3;
}

message FetchChunkManifestResponse {
  // Chunk manifest as arrow RecordBatches
  rerun.common.v1alpha1.DataframePart data = 1;
}

message DoMaintenanceRequest {
  rerun.common.v1alpha1.DatasetHandle entry = 1;
  bool build_scalar_indexes = 2;
  bool compact_fragments = 3;
}

message DoMaintenanceResponse {
  string report = 1;
}

// ----------------- Error handling -----------------

// Application level error - used as `details` in the `google.rpc.Status` message
message Error {
  // error code
  ErrorCode code = 1;
  // unique identifier associated with the request (e.g. recording id, recording storage url)
  string id = 2;
  // human readable details about the error
  string message = 3;
}

// Error codes for application level errors
enum ErrorCode {
  // unused
  ERROR_CODE_UNSPECIFIED = 0;

  // object store access error
  ERROR_CODE_OBJECT_STORE_ERROR = 1;

  // metadata database access error
  ERROR_CODE_METADATA_DB_ERROR = 2;

  // Encoding / decoding error
  ERROR_CODE_CODEC_ERROR = 3;
}
