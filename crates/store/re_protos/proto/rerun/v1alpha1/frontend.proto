syntax = "proto3";

package rerun.frontend.v1alpha1;

import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";
import "rerun/v1alpha1/catalog.proto";
import "rerun/v1alpha1/common.proto";
import "rerun/v1alpha1/manifest_registry.proto";

// TODO: what becomes of this comment?
// TODO(cmc): I've decided to re-use the underlying response types for now, in the name of expediency.
// This is bad in the long term (breaking changes to the internal APIs should not result in breaking
// changes to the public API and, in general, the two being coupled doesn't make much sense).
// It's the very early days though, and we're doing major breaking changes on a daily basis, so wasting
// time on patching `From` implementations on a regular basis doesn't sound like a great use of time
// at the moment.
// We can revisit this as we start getting out of `v1alpha1`.

// TODO: this is a horrible name for our product's API
// Redap's public API.
service FrontendService {
  rpc Version(VersionRequest) returns (VersionResponse) {}

  // --- Catalog ---
  // Mostly a 1:1 proxy at the moment.

  rpc FindEntries(rerun.catalog.v1alpha1.FindEntriesRequest) returns (rerun.catalog.v1alpha1.FindEntriesResponse) {}

  rpc DeleteEntry(rerun.catalog.v1alpha1.DeleteEntryRequest) returns (rerun.catalog.v1alpha1.DeleteEntryResponse) {}

  rpc UpdateEntry(rerun.catalog.v1alpha1.UpdateEntryRequest) returns (rerun.catalog.v1alpha1.UpdateEntryResponse) {}

  rpc CreateDatasetEntry(rerun.catalog.v1alpha1.CreateDatasetEntryRequest) returns (rerun.catalog.v1alpha1.CreateDatasetEntryResponse) {}
  rpc ReadDatasetEntry(rerun.catalog.v1alpha1.ReadDatasetEntryRequest) returns (rerun.catalog.v1alpha1.ReadDatasetEntryResponse) {}
  rpc UpdateDatasetEntry(rerun.catalog.v1alpha1.UpdateDatasetEntryRequest) returns (rerun.catalog.v1alpha1.UpdateDatasetEntryResponse) {}

  rpc ReadTableEntry(rerun.catalog.v1alpha1.ReadTableEntryRequest) returns (rerun.catalog.v1alpha1.ReadTableEntryResponse) {}

  // --- Manifest Registry ---
  // Automatically handles entry/dataset resolution.

  /* Write data */

  // Register new partitions with the Dataset
  rpc RegisterWithDataset(RegisterWithDatasetRequest) returns (rerun.manifest_registry.v1alpha1.RegisterWithDatasetResponse) {}

  // Write chunks to one or more partitions.
  //
  // The partition ID for each individual chunk is extracted from their metadata (`rerun.partition_id`).
  //
  // The destination dataset must be provided in the `x-rerun-dataset-id` header.
  rpc WriteChunks(stream rerun.manifest_registry.v1alpha1.WriteChunksRequest) returns (rerun.manifest_registry.v1alpha1.WriteChunksResponse) {}

  /* Query schemas */

  // Returns the schema of the partition table (i.e. the dataset manifest) itself, *not* the underlying dataset.
  //
  // * To inspect the data of the partition table, use `ScanPartitionTable`.
  // * To retrieve the schema of the underlying dataset, use `GetDatasetSchema` instead.
  rpc GetPartitionTableSchema(GetPartitionTableSchemaRequest) returns (rerun.manifest_registry.v1alpha1.GetPartitionTableSchemaResponse) {}

  // Inspect the contents of the partition table (i.e. the dataset manifest).
  //
  // The returned data will follow the schema specified by `GetPartitionTableSchema`.
  rpc ScanPartitionTable(ScanPartitionTableRequest) returns (stream rerun.manifest_registry.v1alpha1.ScanPartitionTableResponse) {}

  // Returns the schema of the dataset.
  //
  // This is the union of all the schemas from all the underlying partitions. It will contain all the indexes,
  // entities and components present in the dataset.
  rpc GetDatasetSchema(GetDatasetSchemaRequest) returns (rerun.manifest_registry.v1alpha1.GetDatasetSchemaResponse) {}

  /* Indexing */

  // Creates a custom index for a specific column (vector search, full-text search, etc).
  rpc CreateIndex(CreateIndexRequest) returns (rerun.manifest_registry.v1alpha1.CreateIndexResponse) {}

  // Recreate an index with the same configuration but (potentially) new data.
  rpc ReIndex(ReIndexRequest) returns (rerun.manifest_registry.v1alpha1.ReIndexResponse) {}

  /* Queries */

  // Search a previously created index.
  rpc SearchDataset(SearchDatasetRequest) returns (stream rerun.manifest_registry.v1alpha1.SearchDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the matching chunk IDs.
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch the actual chunks themselves, see `GetChunks`.
  //
  // Passing chunk IDs to this method effectively acts as a IF_EXIST filter.
  rpc QueryDataset(QueryDatasetRequest) returns (stream rerun.manifest_registry.v1alpha1.QueryDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the underlying chunks.
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch only the actual chunk IDs rather than the chunks themselves, see `QueryDataset`.
  rpc GetChunks(GetChunksRequest) returns (stream rerun.manifest_registry.v1alpha1.GetChunksResponse) {}

  // --- Table Apis ---
  // TODO(jleibs): This will be replaced / extended by Arrow Flight

  // Register a foreign table as a new table entry in the catalog.
  rpc RegisterTable(rerun.catalog.v1alpha1.RegisterTableRequest) returns (rerun.catalog.v1alpha1.RegisterTableResponse) {}

  rpc GetTableSchema(GetTableSchemaRequest) returns (GetTableSchemaResponse) {}
  rpc ScanTable(ScanTableRequest) returns (stream ScanTableResponse) {}

  // --- Tasks service ---

  // Query the status of submitted tasks
  rpc QueryTasks(QueryTasksRequest) returns (QueryTasksResponse) {}

  // Fetch the output of a completed task
  rpc FetchTaskOutput(FetchTaskOutputRequest) returns (FetchTaskOutputResponse) {}

  // Query the status of submitted tasks as soon as they are no longer pending
  rpc QueryTasksOnCompletion(QueryTasksOnCompletionRequest) returns (stream QueryTasksOnCompletionResponse) {}

  // -- Utilities --

  // Rerun Manifests maintenance operations: scalar index creation, compaction, etc.
  rpc DoMaintenance(DoMaintenanceRequest) returns (rerun.manifest_registry.v1alpha1.DoMaintenanceResponse) {}
}

message VersionRequest {}

message VersionResponse {
  rerun.common.v1alpha1.BuildInfo build_info = 1;
}

// --- Manifest Registry ---

/* Write data */

message RegisterWithDatasetRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
  repeated rerun.manifest_registry.v1alpha1.DataSource data_sources = 2;
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

/* Query schemas */

message GetPartitionTableSchemaRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
}

message ScanPartitionTableRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
  rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message GetDatasetSchemaRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
}

/* Indexing */

message CreateIndexRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // List of specific partitions that will be indexed (all if left empty).
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // List of specific partition layers that will be indexed (all if left empty).
  //
  // If non-empty, this must match the length of `partition_ids`.
  repeated string partition_layers = 5;

  rerun.manifest_registry.v1alpha1.IndexConfig config = 3;

  // Specify behavior when index for a partition was already created.
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 4;
}

message ReIndexRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
}

/* Queries */

message SearchDatasetRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Index column that is queried
  rerun.manifest_registry.v1alpha1.IndexColumn column = 2;

  // Query data - type of data is index specific. Caller must ensure
  // to provide the right type. For vector search this should
  // be a vector of appropriate size, for inverted index this should be a string.
  // Query data is represented as a unit (single row) RecordBatch with 1 column.
  rerun.common.v1alpha1.DataframePart query = 3;

  // Index type specific properties
  rerun.manifest_registry.v1alpha1.IndexQueryProperties properties = 4;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;
}

message QueryDatasetRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Client can specify what partitions are queried. If left unspecified (empty list),
  // all partitions will be queried.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify specific chunk ids to include. If left unspecified (empty list),
  // all chunks that match other query parameters will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty, and set `select_all_entity_paths`,
  // in order to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // If set, the query will cover all existing entity paths.
  //
  // `entity_paths` must be empty, otherwise an error will be raised.
  //
  // Truth table:
  // ```text
  // select_all_entity_paths | entity_paths   | result
  // ------------------------+----------------+--------
  // false                   | []             | valid query, empty results (no entity paths selected)
  // false                   | ['foo', 'bar'] | valid query, 'foo' & 'bar' selected
  // true                    | []             | valid query, all entity paths selected
  // true                    | ['foo', 'bar'] | invalid query, error
  // ```
  bool select_all_entity_paths = 7;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLines:width` index, all of the following
  // would match: `SeriesLines:width`, `Width`, `SeriesLines`, etc.
  repeated string fuzzy_descriptors = 10;

  // If set, static data will be excluded from the results.
  bool exclude_static_data = 8;

  // If set, temporal data will be excluded from the results.
  bool exclude_temporal_data = 9;

  // Generic parameters that will influence the behavior of the Lance scanner.
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;

  rerun.manifest_registry.v1alpha1.Query query = 6;
}

message GetChunksRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Client can specify from which partitions to get chunks. If left unspecified (empty list),
  // data from all partition (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify chunk ids to include. If left unspecified (empty list),
  // all chunks (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty, and set `select_all_entity_paths`,
  // in order to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // If set, the query will cover all existing entity paths.
  //
  // `entity_paths` must be empty, otherwise an error will be raised.
  //
  // Truth table:
  // ```text
  // select_all_entity_paths | entity_paths   | result
  // ------------------------+----------------+--------
  // false                   | []             | valid query, empty results (no entity paths selected)
  // false                   | ['foo', 'bar'] | valid query, 'foo' & 'bar' selected
  // true                    | []             | valid query, all entity paths selected
  // true                    | ['foo', 'bar'] | invalid query, error
  // ```
  bool select_all_entity_paths = 6;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLines:width` index, all of the following
  // would match: `SeriesLines:width`, `Width`, `SeriesLines`, etc.
  repeated string fuzzy_descriptors = 9;

  // If set, static data will be excluded from the results.
  bool exclude_static_data = 7;

  // If set, temporal data will be excluded from the results.
  bool exclude_temporal_data = 8;

  // Query details
  rerun.manifest_registry.v1alpha1.Query query = 5;
}

// --- Table Apis ---

message GetTableSchemaRequest {
  rerun.common.v1alpha1.EntryId table_id = 1;
}

message GetTableSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

message ScanTableRequest {
  rerun.common.v1alpha1.EntryId table_id = 1;
  // TODO(jleibs): support ScanParameters iff we can plumb them into Datafusion TableProvider
  // Otherwise, just wait for Arrow Flight
  //rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message ScanTableResponse {
  rerun.common.v1alpha1.DataframePart dataframe_part = 1;
}

message DoMaintenanceRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Create the acceleration structures for temporal queries.
  //
  // This will recreate all scalar indexes from scratch everytime.
  //
  // TODO(cmc): support incremental scalar indexing & index compaction
  bool build_scalar_indexes = 2;

  // Compact the underlying Lance fragments, for all Rerun Manifests.
  //
  // Hardcoded to the default (optimal) settings.
  bool compact_fragments = 3;

  // If set, all Lance fragments older than this date will be removed, for all Rerun Manifests.
  // In case requested date is more recent than 1 hour, it will be ignored and 1 hour ago
  // timestamp will be used. This is to prevent still used files (like recent transaction files)
  // to be removed and cause Lance Dataset update issues.
  // See https://docs.rs/lance/latest/lance/dataset/cleanup/index.html
  // and https://docs.rs/lance/latest/lance/dataset/cleanup/fn.cleanup_old_versions.html
  google.protobuf.Timestamp cleanup_before = 4;

  // Override default platform behavior and allow cleanup of recent files. This will respect
  // the value of `cleanup_before` timestamp even if it's more recent than 1 hour.
  // ⚠️ Do not ever use this unless you know exactly what you're doing. Improper use will lead to data loss.
  bool unsafe_allow_recent_cleanup = 5;
}

// --- Tasks ---

// A task is a unit of work that can be submitted to the system
message Task {
  // Unique identifier for the task
  rerun.common.v1alpha1.TaskId id = 1;
  // Type of the task
  string task_type = 2;
  // Task-type dependant data necessary to de-serialize the task
  bytes task_data = 3;
}

// `SubmitTasksRequest` is the request message for submitting tasks
message SubmitTasksRequest {
  repeated Task tasks = 1;
}

// `SubmitTaskResponse` contains, for each submitted task
// its submission outcome, encoded as a `RecordBatch`
message SubmitTasksResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// `QueryTasksRequest` is the request message for querying tasks status
message QueryTasksRequest {
  // Empty queries for all tasks if the server allows it.
  repeated rerun.common.v1alpha1.TaskId ids = 1;
}

// `QueryTasksResponse` is the response message for querying tasks status
// encoded as a record batch
message QueryTasksResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// `QueryTasksOnCompletionRequest` is the request message for querying tasks status.
// This is close-to-a-copy of `QueryTasksRequest`, with the addition of a timeout.
message QueryTasksOnCompletionRequest {
  // Empty queries for all tasks if the server allows it.
  repeated rerun.common.v1alpha1.TaskId ids = 1;
  // Time limit for the server to wait for task completion.
  // The actual maximum time may be arbitrarily capped by the server.
  google.protobuf.Duration timeout = 2;
}

// `QueryTaskOnCompletionResponse` is the response message for querying tasks status
// encoded as a record batch. This is a copy of `QueryTasksResponse`.
message QueryTasksOnCompletionResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// `FetchTaskOutputRequest` is the request message for fetching task output
message FetchTaskOutputRequest {
  // Unique identifier for the task
  rerun.common.v1alpha1.TaskId id = 1;
}

/// `FetchTaskOutputResponse` is the response message for fetching task output
message FetchTaskOutputResponse {
  // The output of the task, encoded as a record batch
  rerun.common.v1alpha1.DataframePart data = 1;
}
