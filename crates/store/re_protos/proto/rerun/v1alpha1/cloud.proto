syntax = "proto3";

package rerun.cloud.v1alpha1;

import "google/protobuf/any.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";
import "rerun/v1alpha1/common.proto";
import "rerun/v1alpha1/log_msg.proto";

// The Rerun Cloud public API.
service RerunCloudService {
  rpc Version(VersionRequest) returns (VersionResponse) {}

  // --- Catalog ---

  rpc FindEntries(FindEntriesRequest) returns (FindEntriesResponse) {}

  rpc DeleteEntry(DeleteEntryRequest) returns (DeleteEntryResponse) {}

  rpc UpdateEntry(UpdateEntryRequest) returns (UpdateEntryResponse) {}

  rpc CreateDatasetEntry(CreateDatasetEntryRequest) returns (CreateDatasetEntryResponse) {}

  rpc ReadDatasetEntry(ReadDatasetEntryRequest) returns (ReadDatasetEntryResponse) {}

  rpc UpdateDatasetEntry(UpdateDatasetEntryRequest) returns (UpdateDatasetEntryResponse) {}

  rpc ReadTableEntry(ReadTableEntryRequest) returns (ReadTableEntryResponse) {}

  // --- Manifest Registry ---

  /* Write data */

  // Register new partitions with the Dataset
  rpc RegisterWithDataset(RegisterWithDatasetRequest) returns (RegisterWithDatasetResponse) {}

  // Write chunks to one or more partitions.
  //
  // The partition ID for each individual chunk is extracted from their metadata (`rerun.partition_id`).
  //
  // The destination dataset must be provided in the `x-rerun-dataset-id` header.
  rpc WriteChunks(stream WriteChunksRequest) returns (WriteChunksResponse) {}

  /* Query schemas */

  // Returns the schema of the partition table (i.e. the dataset manifest) itself, *not* the underlying dataset.
  //
  // * To inspect the data of the partition table, use `ScanPartitionTable`.
  // * To retrieve the schema of the underlying dataset, use `GetDatasetSchema` instead.
  rpc GetPartitionTableSchema(GetPartitionTableSchemaRequest) returns (GetPartitionTableSchemaResponse) {}

  // Inspect the contents of the partition table (i.e. the dataset manifest).
  //
  // The returned data will follow the schema specified by `GetPartitionTableSchema`.
  rpc ScanPartitionTable(ScanPartitionTableRequest) returns (stream ScanPartitionTableResponse) {}

  // Returns the schema of the dataset.
  //
  // This is the union of all the schemas from all the underlying partitions. It will contain all the indexes,
  // entities and components present in the dataset.
  rpc GetDatasetSchema(GetDatasetSchemaRequest) returns (GetDatasetSchemaResponse) {}

  /* Indexing */

  // Creates a custom index for a specific column (vector search, full-text search, etc).
  rpc CreateIndex(CreateIndexRequest) returns (CreateIndexResponse) {}

  // Recreate an index with the same configuration but (potentially) new data.
  rpc ReIndex(ReIndexRequest) returns (ReIndexResponse) {}

  /* Queries */

  // Search a previously created index.
  rpc SearchDataset(SearchDatasetRequest) returns (stream SearchDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the matching chunk IDs, as well
  // as information that can be sent back to Rerun Cloud to fetch the actual chunks as part
  // of `FetchChunks` request. In this 2-step query process, 1st step is getting information
  // from the server about the chunks that contain relevant information. 2nd step is fetching
  // those chunks (the actual data).
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch the actual chunks themselves, see `GetChunks`.
  //
  // Passing chunk IDs to this method effectively acts as a IF_EXIST filter.
  rpc QueryDataset(QueryDatasetRequest) returns (stream QueryDatasetResponse) {}

  // Perform Rerun-native queries on a dataset, returning the underlying chunks.
  //
  // These Rerun-native queries include:
  // * Filtering by specific partition and chunk IDs.
  // * Latest-at, range and dataframe queries.
  // * Arbitrary Lance filters.
  //
  // To fetch only the actual chunk IDs rather than the chunks themselves, see `QueryDataset`.
  rpc GetChunks(GetChunksRequest) returns (stream GetChunksResponse) {}

  // Fetch specific chunks from Rerun Cloud. In a 2-step query process, result of 1st phase,
  // that is, the result of `QueryDataset` should include all the necessary information to send
  // the actual chunk requests, which is the 2nd step of the query process.
  //
  // See `FetchChunksRequest` for details on the fields that describe each individual chunk.
  rpc FetchChunks(FetchChunksRequest) returns (stream FetchChunksResponse) {}

  // --- Tables ---
  // TODO(jleibs): This will be replaced / extended by Arrow Flight

  // Register a foreign table as a new table entry in the catalog.
  rpc RegisterTable(RegisterTableRequest) returns (RegisterTableResponse) {}

  rpc GetTableSchema(GetTableSchemaRequest) returns (GetTableSchemaResponse) {}

  rpc ScanTable(ScanTableRequest) returns (stream ScanTableResponse) {}

  // --- Tasks ---

  // Query the status of submitted tasks
  rpc QueryTasks(QueryTasksRequest) returns (QueryTasksResponse) {}

  // Fetch the output of a completed task
  rpc FetchTaskOutput(FetchTaskOutputRequest) returns (FetchTaskOutputResponse) {}

  // Query the status of submitted tasks as soon as they are no longer pending
  rpc QueryTasksOnCompletion(QueryTasksOnCompletionRequest) returns (stream QueryTasksOnCompletionResponse) {}

  // --- Utilities ---

  // Rerun Manifests maintenance operations: scalar index creation, compaction, etc.
  rpc DoMaintenance(DoMaintenanceRequest) returns (DoMaintenanceResponse) {}
}

// ---

message VersionRequest {}

message VersionResponse {
  rerun.common.v1alpha1.BuildInfo build_info = 1;
}

// Application level error - used as `details` in the `google.rpc.Status` message
message Error {
  // error code
  ErrorCode code = 1;
  // unique identifier associated with the request (e.g. recording id, recording storage url)
  string id = 2;
  // human readable details about the error
  string message = 3;
}

// Error codes for application level errors
enum ErrorCode {
  // unused
  ERROR_CODE_UNSPECIFIED = 0;

  // object store access error
  ERROR_CODE_OBJECT_STORE_ERROR = 1;

  // metadata database access error
  ERROR_CODE_METADATA_DB_ERROR = 2;

  // Encoding / decoding error
  ERROR_CODE_CODEC_ERROR = 3;
}

// --- Manifest Registry ---

/* Write data */

message DataSource {
  // Where is the data for this data source stored (e.g. s3://bucket/file or file:///path/to/file)?
  optional string storage_url = 1;

  /// Which Partition Layer should this data source be registered to?
  ///
  /// Defaults to `base` if unspecified.
  optional string layer = 3;

  // What kind of data is it (e.g. rrd, mcap, Lance, etc)?
  DataSourceKind typ = 2;
}

enum DataSourceKind {
  DATA_SOURCE_KIND_UNSPECIFIED = 0;
  DATA_SOURCE_KIND_RRD = 1;
}

message RegisterWithDatasetRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
  repeated DataSource data_sources = 2;
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 3;
}

message RegisterWithDatasetResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message WriteChunksRequest {
  rerun.common.v1alpha1.RerunChunk chunk = 1;
}

message WriteChunksResponse {}

/* Query schemas */

message GetPartitionTableSchemaRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
}

message GetPartitionTableSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

message ScanPartitionTableRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
  rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message ScanPartitionTableResponse {
  // Partitions metadata as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message GetDatasetSchemaRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
}

message GetDatasetSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

/* Indexing */

message CreateIndexRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // List of specific partitions that will be indexed (all if left empty).
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // List of specific partition layers that will be indexed (all if left empty).
  //
  // If non-empty, this must match the length of `partition_ids`.
  repeated string partition_layers = 5;

  IndexConfig config = 3;

  // Specify behavior when index for a partition was already created.
  rerun.common.v1alpha1.IfDuplicateBehavior on_duplicate = 4;
}

message CreateIndexResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message ReIndexRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;
}

message ReIndexResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message IndexConfig {
  // what kind of index do we want to create and what are its index specific properties.
  IndexProperties properties = 1;

  // Component / column we want to index.
  IndexColumn column = 2;

  // What is the filter index i.e. timeline for which we will query the timepoints.
  //
  // TODO(zehiko) this might go away and we might just index across all the timelines
  rerun.common.v1alpha1.IndexColumnSelector time_index = 3;
}

// used to define which column we want to index
message IndexColumn {
  // The path of the entity.
  rerun.common.v1alpha1.EntityPath entity_path = 1;

  // Component details
  rerun.common.v1alpha1.ComponentDescriptor component = 2;
}

message IndexProperties {
  oneof props {
    InvertedIndex inverted = 1;
    VectorIvfPqIndex vector = 2;
    BTreeIndex btree = 3;
  }
}

message InvertedIndex {
  optional bool store_position = 1;
  optional string base_tokenizer = 2;
  // TODO(zehiko) add other properties as needed
}

message VectorIvfPqIndex {
  optional uint32 num_partitions = 1;
  optional uint32 num_sub_vectors = 2;
  VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
  VECTOR_DISTANCE_METRIC_UNSPECIFIED = 0;
  VECTOR_DISTANCE_METRIC_L2 = 1;
  VECTOR_DISTANCE_METRIC_COSINE = 2;
  VECTOR_DISTANCE_METRIC_DOT = 3;
  VECTOR_DISTANCE_METRIC_HAMMING = 4;
}

message BTreeIndex {
  // TODO(zehiko) add properties as needed
}

/* Queries */

message SearchDatasetRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Index column that is queried
  IndexColumn column = 2;

  // Query data - type of data is index specific. Caller must ensure
  // to provide the right type. For vector search this should
  // be a vector of appropriate size, for inverted index this should be a string.
  // Query data is represented as a unit (single row) RecordBatch with 1 column.
  rerun.common.v1alpha1.DataframePart query = 3;

  // Index type specific properties
  IndexQueryProperties properties = 4;

  // Scan parameters
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;
}

message SearchDatasetResponse {
  // Chunks as arrow RecordBatch
  rerun.common.v1alpha1.DataframePart data = 1;
}

message IndexQueryProperties {
  // specific index query properties based on the index type
  oneof props {
    InvertedIndexQuery inverted = 1;
    VectorIndexQuery vector = 2;
    BTreeIndexQuery btree = 3;
  }
}

message InvertedIndexQuery {
  // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
  optional uint32 top_k = 1;
}

message BTreeIndexQuery {
  // TODO(zehiko) add properties as needed
}

message QueryDatasetRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Client can specify what partitions are queried. If left unspecified (empty list),
  // all partitions will be queried.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify specific chunk ids to include. If left unspecified (empty list),
  // all chunks that match other query parameters will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty, and set `select_all_entity_paths`,
  // in order to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // If set, the query will cover all existing entity paths.
  //
  // `entity_paths` must be empty, otherwise an error will be raised.
  //
  // Truth table:
  // ```text
  // select_all_entity_paths | entity_paths   | result
  // ------------------------+----------------+--------
  // false                   | []             | valid query, empty results (no entity paths selected)
  // false                   | ['foo', 'bar'] | valid query, 'foo' & 'bar' selected
  // true                    | []             | valid query, all entity paths selected
  // true                    | ['foo', 'bar'] | invalid query, error
  // ```
  bool select_all_entity_paths = 7;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLines:width` index, all of the following
  // would match: `SeriesLines:width`, `Width`, `SeriesLines`, etc.
  repeated string fuzzy_descriptors = 10;

  // If set, static data will be excluded from the results.
  bool exclude_static_data = 8;

  // If set, temporal data will be excluded from the results.
  bool exclude_temporal_data = 9;

  // Generic parameters that will influence the behavior of the Lance scanner.
  rerun.common.v1alpha1.ScanParameters scan_parameters = 5;

  Query query = 6;
}

message QueryDatasetResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

message Query {
  // If specified, will perform a latest-at query with the given parameters.
  //
  // You can combine this with a `QueryRange` in order to gather all the relevant chunks for
  // a full-fledged dataframe query (i.e. they get OR'd together).
  QueryLatestAt latest_at = 1;

  // If specified, will perform a range query with the given parameters.
  //
  // You can combine this with a `QueryLatestAt` in order to gather all the relevant chunks for
  // a full-fledged dataframe query (i.e. they get OR'd together).
  QueryRange range = 2;

  // If true, `columns` will contain the entire schema.
  bool columns_always_include_everything = 3;

  // If true, `columns` always includes `chunk_id`,
  bool columns_always_include_chunk_ids = 4;

  // If true, `columns` always includes `byte_offset` and `byte_size`.
  bool columns_always_include_byte_offsets = 5;

  // If true, `columns` always includes `entity_path`.
  bool columns_always_include_entity_paths = 6;

  // If true, `columns` always includes all static component-level indexes.
  bool columns_always_include_static_indexes = 7;

  // If true, `columns` always includes all temporal chunk-level indexes.
  bool columns_always_include_global_indexes = 8;

  // If true, `columns` always includes all component-level indexes.
  bool columns_always_include_component_indexes = 9;
}

// A chunk-level latest-at query, aka `LatestAtRelevantChunks`.
//
// This has the exact same semantics as the query of the same name on our `ChunkStore`.
message QueryLatestAt {
  // Which index column should we perform the query on? E.g. `log_time`.
  //
  // Leave this empty to query for static data.
  rerun.common.v1alpha1.IndexColumnSelector index = 1;

  // What index value are we looking for?
  //
  // Leave this empty to query for static data.
  optional int64 at = 2;

  reserved 3;
  reserved "fuzzy_descriptors";
}

/// A chunk-level range query, aka `RangeRelevantChunks`.
//
// This has the exact same semantics as the query of the same name on our `ChunkStore`.
message QueryRange {
  // Which index column should we perform the query on? E.g. `log_time`.
  rerun.common.v1alpha1.IndexColumnSelector index = 1;

  // What index range are we looking for?
  rerun.common.v1alpha1.TimeRange index_range = 2;

  reserved 3;
  reserved "fuzzy_descriptors";
}

message GetChunksRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Client can specify from which partitions to get chunks. If left unspecified (empty list),
  // data from all partition (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.PartitionId partition_ids = 2;

  // Client can specify chunk ids to include. If left unspecified (empty list),
  // all chunks (that match other query parameters) will be included.
  repeated rerun.common.v1alpha1.Tuid chunk_ids = 3;

  // Which entity paths are we interested in? Leave empty, and set `select_all_entity_paths`,
  // in order to query all of them.
  repeated rerun.common.v1alpha1.EntityPath entity_paths = 4;

  // If set, the query will cover all existing entity paths.
  //
  // `entity_paths` must be empty, otherwise an error will be raised.
  //
  // Truth table:
  // ```text
  // select_all_entity_paths | entity_paths   | result
  // ------------------------+----------------+--------
  // false                   | []             | valid query, empty results (no entity paths selected)
  // false                   | ['foo', 'bar'] | valid query, 'foo' & 'bar' selected
  // true                    | []             | valid query, all entity paths selected
  // true                    | ['foo', 'bar'] | invalid query, error
  // ```
  bool select_all_entity_paths = 6;

  // Which components are we interested in?
  //
  // If left unspecified, all existing components are considered of interest.
  //
  // This will perform a basic fuzzy match on the available columns' descriptors.
  // The fuzzy logic is a simple case-sensitive `contains()` query.
  // For example, given a `log_tick__SeriesLines:width` index, all of the following
  // would match: `SeriesLines:width`, `Width`, `SeriesLines`, etc.
  repeated string fuzzy_descriptors = 9;

  // If set, static data will be excluded from the results.
  bool exclude_static_data = 7;

  // If set, temporal data will be excluded from the results.
  bool exclude_temporal_data = 8;

  // Query details
  Query query = 5;
}

message GetChunksResponse {
  // Every gRPC response, even within the confines of a stream, involves HTTP2 overhead, which isn't
  // cheap by any means, which is why we're returning a batch of `ArrowMsg` rather than a single one.
  repeated rerun.log_msg.v1alpha1.ArrowMsg chunks = 1;
}

message FetchChunksRequest {
  // Information about the chunks to fetch. These dataframes have to include the following columns:
  // * `chunk_id` - Chunk unique identifier
  // * `partition_id` - partition this chunk belongs to. Currently needed as we pass this metadata back and forth
  // * `partition_layer` - specific partition layer. Currently needed as we pass this metadata back and forth
  // * `chunk_key` - chunk location details
  repeated rerun.common.v1alpha1.DataframePart chunk_infos = 1;
}

message FetchChunksResponse {
  // Every gRPC response, even within the confines of a stream, involves HTTP2 overhead, which isn't
  // cheap by any means, which is why we're returning a batch of `ArrowMsg` rather than a single one.
  repeated rerun.log_msg.v1alpha1.ArrowMsg chunks = 1;
}

// --- Table Apis ---

message GetTableSchemaRequest {
  rerun.common.v1alpha1.EntryId table_id = 1;
}

message GetTableSchemaResponse {
  rerun.common.v1alpha1.Schema schema = 1;
}

message ScanTableRequest {
  rerun.common.v1alpha1.EntryId table_id = 1;
  // TODO(jleibs): support ScanParameters iff we can plumb them into Datafusion TableProvider
  // Otherwise, just wait for Arrow Flight
  //rerun.common.v1alpha1.ScanParameters scan_parameters = 2;
}

message ScanTableResponse {
  rerun.common.v1alpha1.DataframePart dataframe_part = 1;
}

message DoMaintenanceRequest {
  rerun.common.v1alpha1.EntryId dataset_id = 1;

  // Create the acceleration structures for temporal queries.
  //
  // This will recreate all scalar indexes from scratch everytime.
  //
  // TODO(cmc): support incremental scalar indexing & index compaction
  bool build_scalar_indexes = 2;

  // Compact the underlying Lance fragments, for all Rerun Manifests.
  //
  // Hardcoded to the default (optimal) settings.
  bool compact_fragments = 3;

  // If set, all Lance fragments older than this date will be removed, for all Rerun Manifests.
  // In case requested date is more recent than 1 hour, it will be ignored and 1 hour ago
  // timestamp will be used. This is to prevent still used files (like recent transaction files)
  // to be removed and cause Lance Dataset update issues.
  // See https://docs.rs/lance/latest/lance/dataset/cleanup/index.html
  // and https://docs.rs/lance/latest/lance/dataset/cleanup/fn.cleanup_old_versions.html
  google.protobuf.Timestamp cleanup_before = 4;

  // Override default platform behavior and allow cleanup of recent files. This will respect
  // the value of `cleanup_before` timestamp even if it's more recent than 1 hour.
  // ⚠️ Do not ever use this unless you know exactly what you're doing. Improper use will lead to data loss.
  bool unsafe_allow_recent_cleanup = 5;
}

message DoMaintenanceResponse {
  string report = 1;
}

// --- Tasks ---

// A task is a unit of work that can be submitted to the system
message Task {
  // Unique identifier for the task
  rerun.common.v1alpha1.TaskId id = 1;
  // Type of the task
  string task_type = 2;
  // Task-type dependant data necessary to de-serialize the task
  bytes task_data = 3;
}

// `SubmitTasksRequest` is the request message for submitting tasks
message SubmitTasksRequest {
  repeated Task tasks = 1;
}

// `SubmitTaskResponse` contains, for each submitted task
// its submission outcome, encoded as a `RecordBatch`
message SubmitTasksResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// `QueryTasksRequest` is the request message for querying tasks status
message QueryTasksRequest {
  // Empty queries for all tasks if the server allows it.
  repeated rerun.common.v1alpha1.TaskId ids = 1;
}

// `QueryTasksResponse` is the response message for querying tasks status
// encoded as a record batch
message QueryTasksResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// `QueryTasksOnCompletionRequest` is the request message for querying tasks status.
// This is close-to-a-copy of `QueryTasksRequest`, with the addition of a timeout.
message QueryTasksOnCompletionRequest {
  // Empty queries for all tasks if the server allows it.
  repeated rerun.common.v1alpha1.TaskId ids = 1;
  // Time limit for the server to wait for task completion.
  // The actual maximum time may be arbitrarily capped by the server.
  google.protobuf.Duration timeout = 2;
}

// `QueryTaskOnCompletionResponse` is the response message for querying tasks status
// encoded as a record batch. This is a copy of `QueryTasksResponse`.
message QueryTasksOnCompletionResponse {
  rerun.common.v1alpha1.DataframePart data = 1;
}

// `FetchTaskOutputRequest` is the request message for fetching task output
message FetchTaskOutputRequest {
  // Unique identifier for the task
  rerun.common.v1alpha1.TaskId id = 1;
}

/// `FetchTaskOutputResponse` is the response message for fetching task output
message FetchTaskOutputResponse {
  // The output of the task, encoded as a record batch
  rerun.common.v1alpha1.DataframePart data = 1;
}

// --- Catalog ---

// FindEntries

message FindEntriesRequest {
  EntryFilter filter = 1;
}

message FindEntriesResponse {
  repeated EntryDetails entries = 1;
}

// DeleteDatasetEntry

message DeleteEntryRequest {
  rerun.common.v1alpha1.EntryId id = 1;
}

message DeleteEntryResponse {}

// UpdateEntry

message UpdateEntryRequest {
  // The entry to modify.
  rerun.common.v1alpha1.EntryId id = 1;

  // The new values for updatable fields.
  EntryDetailsUpdate entry_details_update = 2;
}

message UpdateEntryResponse {
  // The updated entry details
  EntryDetails entry_details = 1;
}

// CreateDatasetEntry

message CreateDatasetEntryRequest {
  // Name of the dataset entry to create.
  //
  // The name should be a short human-readable string. It must be unique within all entries in the catalog. If an entry
  // with the same name already exists, the request will fail.
  optional string name = 1;

  // If specified, create the entry using this specific ID. Use at your own risk.
  optional rerun.common.v1alpha1.EntryId id = 2;

  //TODO(ab): add IfExistingBehavior?
}

message CreateDatasetEntryResponse {
  DatasetEntry dataset = 1;
}

// ReadDatasetEntry

message ReadDatasetEntryRequest {
  rerun.common.v1alpha1.EntryId id = 1;
}

message ReadDatasetEntryResponse {
  DatasetEntry dataset = 1;
}

// UpdateDatasetEntry

message UpdateDatasetEntryRequest {
  // The dataset to modify.
  rerun.common.v1alpha1.EntryId id = 1;

  // The new values.
  DatasetDetails dataset_details = 2;
}

message UpdateDatasetEntryResponse {
  // The updated dataset entry
  DatasetEntry dataset = 1;
}

// RegisterTable

message RegisterTableRequest {
  // Name of the table entry to create.
  //
  // The name should be a short human-readable string. It must be unique within all entries in the catalog. If an entry
  // with the same name already exists, the request will fail.
  string name = 1;

  // Information about the table to register.
  //
  // This must be encoded message of one one of the following supported types:
  // - LanceTable
  google.protobuf.Any provider_details = 2;

  //TODO(ab): add IfExistingBehavior?
}

message RegisterTableResponse {
  // Details about the table that was created and registered.
  TableEntry table_entry = 1;
}

// ReadTableEntry

message ReadTableEntryRequest {
  rerun.common.v1alpha1.EntryId id = 1;
}

message ReadTableEntryResponse {
  TableEntry table = 1;
}

message EntryFilter {
  optional rerun.common.v1alpha1.EntryId id = 1;
  optional string name = 2;
  optional EntryKind entry_kind = 3;
}

// What type of entry. This has strong implication on which APIs are available for this entry.
enum EntryKind {
  // Always reserve unspecified as default value
  ENTRY_KIND_UNSPECIFIED = 0;

  // Order as TYPE, TYPE_VIEW so things stay consistent as we introduce new types.
  ENTRY_KIND_DATASET = 1;

  ENTRY_KIND_DATASET_VIEW = 2;

  ENTRY_KIND_TABLE = 3;

  ENTRY_KIND_TABLE_VIEW = 4;

  ENTRY_KIND_BLUEPRINT_DATASET = 5;
}

// Minimal info about an Entry for high-level catalog summary
message EntryDetails {
  // The EntryId is immutable
  rerun.common.v1alpha1.EntryId id = 1;

  // The name of this entry.
  optional string name = 2;

  // The kind of entry
  EntryKind entry_kind = 3;

  google.protobuf.Timestamp created_at = 4;
  google.protobuf.Timestamp updated_at = 5;
}

// Updatable fields of an Entry
message EntryDetailsUpdate {
  // The name of this entry.
  optional string name = 2;
}

message DatasetDetails {
  // The blueprint dataset associated with this dataset (if any).
  optional rerun.common.v1alpha1.EntryId blueprint_dataset = 3;

  // The partition of the blueprint dataset corresponding to the default blueprint (if any).
  optional rerun.common.v1alpha1.PartitionId default_blueprint = 4;
}

message DatasetEntry {
  reserved 3;
  reserved "blueprint_dataset";

  EntryDetails details = 1;

  // Dataset-specific information, may be update with `UpdateDatasetEntry`
  DatasetDetails dataset_details = 4;

  // Read-only
  rerun.common.v1alpha1.DatasetHandle dataset_handle = 2;
}

message TableEntry {
  EntryDetails details = 1;

  // Details specific to the table-provider
  google.protobuf.Any provider_details = 3;

  reserved 2;
  reserved "schema";
}

enum SystemTableKind {
  // Always reserve unspecified as default value
  SYSTEM_TABLE_KIND_UNSPECIFIED = 0;
  // Not used yet
  SYSTEM_TABLE_KIND_NAMESPACES = 1;
  // All of the entries in the associated namespace
  SYSTEM_TABLE_KIND_ENTRIES = 2;
}

message SystemTable {
  SystemTableKind kind = 1;
}

// A foreign table stored as a Lance table.
message LanceTable {
  // The URL of the Lance table.
  string table_url = 1;
}
