syntax = "proto3";

package rerun.remote_store.v0;

import "rerun/v0/common.proto";

service StorageNode {
    // Data APIs

    rpc Query(QueryRequest) returns (stream DataframePart) {}

    rpc FetchRecording(FetchRecordingRequest) returns (stream rerun.common.v0.RerunChunk) {}

    // Index APIs

    rpc CreateIndex(CreateIndexRequest) returns (CreateIndexResponse) {}

    rpc ReIndex(ReIndexRequest) returns (ReIndexResponse) {}

    rpc GetChunkIds(GetChunkIdsRequest) returns (stream GetChunkIdsResponse) {}

    rpc GetChunks(GetChunksRequest) returns (stream rerun.common.v0.RerunChunk) {}

    // The response to `SearchIndex` a RecordBatch with 3 columns:
    // - 'resource_id' column with the id of the resource
    // - timepoint column with the values representing the points in time
    // where index query matches. What time points are matched depends on the type of
    // index that is queried. For example for vector search it might be timepoints where
    // top-K matches are found within *each* resource in the indexed entry. For inverted index
    // it might be timepoints where the query string is found in the indexed column
    // - 'data' column with the data that is returned for the matched timepoints
    rpc SearchIndex(SearchIndexRequest) returns (stream DataframePart) {}

    // Chunk manifest APIs

    rpc CreateManifests(CreateManifestsRequest) returns (CreateManifestsResponse) {}

    rpc ListManifests(ListManifestsRequest) returns (stream DataframePart) {}

    rpc QueryManifest(QueryManifestRequest) returns (stream DataframePart) {}

    // TODO(zehiko, cmc): DeleteManifest

    // Metadata APIs

    rpc QueryCatalog(QueryCatalogRequest) returns (stream DataframePart) {}

    rpc UpdateCatalog(UpdateCatalogRequest) returns (UpdateCatalogResponse) {}

    rpc GetRecordingSchema(GetRecordingSchemaRequest) returns (GetRecordingSchemaResponse) {}

    // Registration APIs

    // TODO(zehiko) support registering more than one recording at a time
    rpc RegisterRecording(RegisterRecordingRequest) returns (DataframePart) {}

    rpc UnregisterRecording(UnregisterRecordingRequest) returns (UnregisterRecordingResponse) {}

    rpc UnregisterAllRecordings(UnregisterAllRecordingsRequest)
        returns (UnregisterAllRecordingsResponse) {}
}

// ---------------- Common response message ------------------

// DataframePart is arrow IPC encoded RecordBatch
message DataframePart {
    // encoder version used to encode the data
    rerun.common.v0.EncoderVersion encoder_version = 1;

    // Data payload is Arrow IPC encoded RecordBatch
    bytes payload = 1000;
}

// ---------------- GetChunkIds ------------------

message GetChunkIdsRequest {
    // recording id from which we're want to fetch the chunk ids
    rerun.common.v0.RecordingId recording_id = 1;
    // timeline for which we specify the time range
    rerun.common.v0.IndexColumnSelector time_index = 2;
    // time range for which we want to fetch the chunk ids
    rerun.common.v0.TimeRange time_range = 3;
}

message GetChunkIdsResponse {
    // a batch of chunk ids for chunks that are within the specified time range
    repeated rerun.common.v0.Tuid chunk_ids = 1;
}

// ---------------- GetChunk ---------------------

message GetChunksRequest {
    // recording id from which we're want to fetch the chunk ids
    rerun.common.v0.RecordingId recording_id = 1;
    // batch of chunk ids for which we want to stream back chunks
    repeated rerun.common.v0.Tuid chunk_ids = 2;
}

// ---------------- CreateIndex ------------------

// used to define which column we want to index
message IndexColumn {
    // The path of the entity.
    rerun.common.v0.EntityPath entity_path = 1;
    // Optional name of the `Archetype` associated with this data.
    optional string archetype_name = 2;
    // Optional name of the field within `Archetype` associated with this data.
    optional string archetype_field_name = 3;
    // Semantic name associated with this data.
    string component_name = 4;
}

message CreateIndexRequest {
    // which catalog entry do we want to create index for
    CatalogEntry entry = 1;
    // what kind of index do we want to create and what are
    // its index specific properties
    IndexProperties properties = 2;
    // Component / column we want to index
    IndexColumn column = 3;
    // What is the filter index i.e. timeline for which we
    // will query the timepoints
    // TODO(zehiko) this might go away and we might just index
    // across all the timelines
    rerun.common.v0.IndexColumnSelector time_index = 4;
}

message ReIndexRequest {
    // which catalog entry do we want to reindex for
    CatalogEntry entry = 1;
    // which column do we want to reindex
    IndexColumn column = 2;
}

message IndexProperties {
    oneof props {
        InvertedIndex inverted = 1;
        VectorIvfPqIndex vector = 2;
        BTreeIndex btree = 3;
    }
}

message InvertedIndex {
    bool store_position = 1;
    string base_tokenizer = 2;
    // TODO(zehiko) add other properties as needed
}

message VectorIvfPqIndex {
    uint32 num_partitions = 1;
    uint32 num_sub_vectors = 2;
    VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
    L2 = 0;
    COSINE = 1;
    DOT = 2;
    HAMMING = 3;
}

message BTreeIndex {
    // TODO(zehiko) add properties as needed
}

message CreateIndexResponse {
    uint64 indexed_rows = 1;
}

message ReIndexResponse {}

// ---------------- SearchIndex ------------------

message SearchIndexRequest {
    // The catalog entry that we want to search over
    // If not specified, the default collection is queried
    CatalogEntry entry = 1;
    // Index column that is queried
    IndexColumn column = 2;
    // Query data - type of data is index specific. Caller must ensure
    // to provide the right type. For vector search this should
    // be a vector of appropriate size, for inverted index this should be a string.
    // Query data is represented as a unit (single row) RecordBatch with 1 column.
    DataframePart query = 3;
    // Index type specific properties
    IndexQueryProperties properties = 4;
    // max number of rows to be returned
    optional uint32 limit = 5;
}

message IndexQueryProperties {
    // specific index query properties based on the index type
    oneof props {
        InvertedIndexQuery inverted = 1;
        VectorIndexQuery vector = 2;
        BTreeIndexQuery btree = 3;
    }
}

message InvertedIndexQuery {
    // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
    uint32 top_k = 2;
}

message BTreeIndexQuery {
    // TODO(zehiko) add properties as needed
}

message CatalogEntry {
    string name = 1;
}

// ---------------- CreateManifests ------------------

// TODO(zehiko, cmc): At some point, this will need to be fully async (i.e. caller gets assigned
// a unique request ID and polls it for completion), but:
// A) Let's wait until we have a real need for this.
// B) That's true of everything in the platform, so this needs to be properly generalized.

message CreateManifestsRequest {
    // Which catalog entry do we want to create manifests for?
    CatalogEntry entry = 1;
}

message CreateManifestsResponse {}

// ---------------- ListManifests ------------------

message ListManifestsRequest {
    // Which catalog entry do we want to list the manifests of?
    CatalogEntry entry = 1;

    // Generic parameters that will influence the behavior of the Lance scanner.
    //
    // TODO(zehiko, cmc): actually support those.
    ScanParameters scan_parameters = 500;
}

message ListManifestsResponse {
    rerun.common.v0.EncoderVersion encoder_version = 1;

    // The record batch of the response, encoded according to `encoder_version`.
    bytes payload = 2;
}

// ---------------- QueryManifest ------------------

// TODO(zehiko, cmc): Being able to specify only a collection ID rather than a resource ID could be
// super useful for cross-recording queries (resource_id becomes a column of the result).

// A manifest query will find all the relevant chunk IDs (and optionally a bunch of related metadata)
// for a given Rerun query (latest-at, range, etc).
//
// The result might contain duplicated chunk IDs, it is the responsibility of the caller to deduplicate
// them as needed.
message QueryManifestRequest {
    // What resource are we querying the manifest for?
    rerun.common.v0.RecordingId resource_id = 100;

    // What columns of the manifest are we interested in?
    ColumnProjection columns = 200;

    // If true, `columns` will contain the entire schema.
    bool columns_always_include_everything = 210;

    // If true, `columns` always includes `chunk_id`,
    bool columns_always_include_chunk_ids = 220;

    // If true, `columns` always includes `byte_offset` and `byte_size`.
    bool columns_always_include_byte_offsets = 230;

    // If true, `columns` always includes all static component-level indexes.
    bool columns_always_include_static_indexes = 240;

    // If true, `columns` always includes all temporal chunk-level indexes.
    bool columns_always_include_global_indexes = 250;

    // If true, `columns` always includes all component-level indexes.
    bool columns_always_include_component_indexes = 260;

    // If specified, will perform a latest-at query with the given parameters.
    //
    // Incompatible with `range`.
    QueryManifestLatestAtRelevantChunks latest_at = 300;

    // If specified, will perform a range query with the given parameters.
    //
    // Incompatible with `latest_at`.
    QueryManifestRangeRelevantChunks range = 400;

    // Generic parameters that will influence the behavior of the Lance scanner.
    ScanParameters scan_parameters = 500;
}

message QueryManifestLatestAtRelevantChunks {
    // Which index column should we perform the query on? E.g. `log_time`.
    rerun.common.v0.IndexColumnSelector index = 1;

    // What index value are we looking for?
    int64 at = 2;

    // Which components are we interested in?
    //
    // If left unspecified, all existing components are considered of interest.
    //
    // This will perform a basic fuzzy match on the available columns' descriptors.
    // The fuzzy logic is a simple case-sensitive `contains()` query.
    // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
    // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
    // `SeriesLine`, etc.
    repeated string fuzzy_descriptors = 3;
}

message QueryManifestRangeRelevantChunks {
    // Which index column should we perform the query on? E.g. `log_time`.
    rerun.common.v0.IndexColumnSelector index = 1;

    // What index range are we looking for?
    rerun.common.v0.TimeRange index_range = 2;

    // Which components are we interested in?
    //
    // If left unspecified, all existing components are considered of interest.
    //
    // This will perform a basic fuzzy match on the available columns' descriptors.
    // The fuzzy logic is a simple case-sensitive `contains()` query.
    // For example, given a `log_tick__SeriesLine:StrokeWidth#width` index, all of the following
    // would match: `SeriesLine:StrokeWidth#width`, `StrokeWidth`, `Stroke`, `Width`, `width`,
    // `SeriesLine`, etc.
    repeated string fuzzy_descriptors = 3;
}

// Generic parameters that will influence the behavior of the Lance scanner.
//
// TODO(zehiko, cmc): This should be available for every endpoint that queries data in
// one way or another.
message ScanParameters {
    // An arbitrary filter expression that will be passed to the Lance scanner as-is.
    //
    // ```text
    // scanner.filter(filter)
    // ```
    string filter = 100;

    // An arbitrary offset that will be passed to the Lance scanner as-is.
    //
    // ```text
    // scanner.limit(_, limit_offset)
    // ```
    int64 limit_offset = 200;

    // An arbitrary limit that will be passed to the Lance scanner as-is.
    //
    // ```text
    // scanner.limit(limit_len, _)
    // ```
    int64 limit_len = 201;

    // An arbitrary order clause that will be passed to the Lance scanner as-is.
    //
    // ```text
    // scanner.order_by(â€¦)
    // ```
    ScanParametersOrderClause order_by = 300;

    // If set, the output of `scanner.explain_plan` will be dumped to the server's log.
    bool explain = 400;
}

message ScanParametersOrderClause {
    bool ascending = 10;
    bool nulls_first = 20;
    string column_name = 30;
}

// ---------------- GetRecordingSchema ------------------

message GetRecordingSchemaRequest {
    rerun.common.v0.RecordingId recording_id = 1;
}

message GetRecordingSchemaResponse {
    rerun.common.v0.Schema schema = 2;
}

// ---------------- RegisterRecording ------------------

message RegisterRecordingRequest {
    // to which catalog entry do we want to register the recording
    CatalogEntry entry = 1;
    // human readable description of the recording
    string description = 2;
    // recording storage url (e.g. s3://bucket/file or file:///path/to/file)
    string storage_url = 3;
    // type of recording
    RecordingType typ = 4;
    // (optional) any additional metadata that should be associated with the recording
    // You can associate any arbtrirary number of columns with a specific recording
    DataframePart metadata = 5;
}

// ---------------- Unregister from catalog ------------------

message UnregisterRecordingRequest {
    // which catalog entry do we want to unregister the recording from
    CatalogEntry entry = 1;
    // unique identifier of the recording
    rerun.common.v0.RecordingId recording_id = 2;
}
message UnregisterRecordingResponse {}

message UnregisterAllRecordingsRequest {
    // which catalog entry do we want to unregister all recordings from
    CatalogEntry entry = 1;
}
message UnregisterAllRecordingsResponse {}

// ---------------- UpdateCatalog  -----------------

message UpdateCatalogRequest {
    DataframePart metadata = 2;
}

message UpdateCatalogResponse {}

// ---------------- Query -----------------

message QueryRequest {
    // unique identifier of the recording
    rerun.common.v0.RecordingId recording_id = 1;
    // query to execute
    rerun.common.v0.Query query = 3;
}

// ----------------- QueryCatalog -----------------

message QueryCatalogRequest {
    // Column projection - define which columns should be returned.
    // Providing it is optional, if not provided, all columns should be returned
    ColumnProjection column_projection = 1;
    // Filter specific recordings that match the criteria (selection)
    CatalogFilter filter = 2;
}

message ColumnProjection {
    repeated string columns = 1;
}

message CatalogFilter {
    // Filtering is very simple right now, we can only select
    // recordings by their ids.
    repeated rerun.common.v0.RecordingId recording_ids = 1;
}

message QueryCatalogResponse {
    rerun.common.v0.EncoderVersion encoder_version = 1;
    // raw bytes are TransportChunks (i.e. RecordBatches) encoded with the relevant codec
    bytes payload = 2;
}

enum RecordingType {
    RRD = 0;
}

// ----------------- FetchRecording -----------------

message FetchRecordingRequest {
    rerun.common.v0.RecordingId recording_id = 1;
}

// TODO(jleibs): Eventually this becomes either query-mediated in some way, but for now
// it's useful to be able to just get back the whole RRD somehow.
message FetchRecordingResponse {
    // TODO(zehiko) we need to expand this to become something like 'encoder options'
    // as we will need to specify additional options like compression, including schema
    // in payload, etc.
    rerun.common.v0.EncoderVersion encoder_version = 1;
    // payload is raw bytes that the relevant codec can interpret
    bytes payload = 2;
}

// ----------------- Error handling -----------------

// Application level error - used as `details` in the `google.rpc.Status` message
message RemoteStoreError {
    // error code
    ErrorCode code = 1;
    // unique identifier associated with the request (e.g. recording id, recording storage url)
    string id = 2;
    // human readable details about the error
    string message = 3;
}

// Error codes for application level errors
enum ErrorCode {
    // unused
    _UNUSED = 0;

    // object store access error
    OBJECT_STORE_ERROR = 1;

    // metadata database access error
    METADATA_DB_ERROR = 2;

    // Encoding / decoding error
    CODEC_ERROR = 3;
}
