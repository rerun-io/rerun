syntax = "proto3";

package rerun.remote_store.v0;

import "rerun/v0/common.proto";

service StorageNode {
    // data API calls
    rpc Query(QueryRequest) returns (stream DataframePart) {}
    rpc FetchRecording(FetchRecordingRequest) returns (stream rerun.common.v0.RerunChunk) {}

    rpc CreateIndex(CreateIndexRequest) returns (CreateIndexResponse) {}
    rpc ReIndex(ReIndexRequest) returns (ReIndexResponse) {}

    rpc GetChunkIds(GetChunkIdsRequest) returns (stream GetChunkIdsResponse) {}
    rpc GetChunks(GetChunksRequest) returns (stream rerun.common.v0.RerunChunk) {}

    // The response to `SearchIndex` a RecordBatch with 3 columns:
    // - 'resource_id' column with the id of the resource
    // - timepoint column with the values representing the points in time
    // where index query matches. What time points are matched depends on the type of
    // index that is queried. For example for vector search it might be timepoints where
    // top-K matches are found within *each* resource in the indexed entry. For inverted index
    // it might be timepoints where the query string is found in the indexed column
    // - 'data' column with the data that is returned for the matched timepoints
    rpc SearchIndex(SearchIndexRequest) returns (stream DataframePart) {}

    // metadata API calls
    rpc QueryCatalog(QueryCatalogRequest) returns (stream DataframePart) {}
    rpc UpdateCatalog(UpdateCatalogRequest) returns (UpdateCatalogResponse) {}
    rpc GetRecordingSchema(GetRecordingSchemaRequest) returns (GetRecordingSchemaResponse) {}

    // TODO(zehiko) support registering more than one recording at a time
    rpc RegisterRecording(RegisterRecordingRequest) returns (DataframePart) {}

    rpc UnregisterRecording(UnregisterRecordingRequest) returns (UnregisterRecordingResponse) {}
    rpc UnregisterAllRecordings(UnregisterAllRecordingsRequest)
        returns (UnregisterAllRecordingsResponse) {}
}

// ---------------- Common response message ------------------

// DataframePart is arrow IPC encoded RecordBatch
message DataframePart {
    // encoder version used to encode the data
    rerun.common.v0.EncoderVersion encoder_version = 1;

    // Data payload is Arrow IPC encoded RecordBatch
    bytes payload = 1000;
}

// ---------------- GetChunkIds ------------------

message GetChunkIdsRequest {
    // recording id from which we're want to fetch the chunk ids
    rerun.common.v0.RecordingId recording_id = 1;
    // timeline for which we specify the time range
    rerun.common.v0.IndexColumnSelector time_index = 2;
    // time range for which we want to fetch the chunk ids
    rerun.common.v0.TimeRange time_range = 3;
}

message GetChunkIdsResponse {
    // a batch of chunk ids for chunks that are within the specified time range
    repeated rerun.common.v0.Tuid chunk_ids = 1;
}

// ---------------- GetChunk ---------------------

message GetChunksRequest {
    // recording id from which we're want to fetch the chunk ids
    rerun.common.v0.RecordingId recording_id = 1;
    // batch of chunk ids for which we want to stream back chunks
    repeated rerun.common.v0.Tuid chunk_ids = 2;
}

// ---------------- CreateIndex ------------------

// used to define which column we want to index
message IndexColumn {
    // The path of the entity.
    rerun.common.v0.EntityPath entity_path = 1;
    // Optional name of the `Archetype` associated with this data.
    optional string archetype_name = 2;
    // Optional name of the field within `Archetype` associated with this data.
    optional string archetype_field_name = 3;
    // Semantic name associated with this data.
    string component_name = 4;
}

message CreateIndexRequest {
    // which catalog entry do we want to create index for
    CatalogEntry entry = 1;
    // what kind of index do we want to create and what are
    // its index specific properties
    IndexProperties properties = 2;
    // Component / column we want to index
    IndexColumn column = 3;
    // What is the filter index i.e. timeline for which we
    // will query the timepoints
    // TODO(zehiko) this might go away and we might just index
    // across all the timelines
    rerun.common.v0.IndexColumnSelector time_index = 4;
}

message ReIndexRequest {
    // which catalog entry do we want to reindex for
    CatalogEntry entry = 1;
    // which column do we want to reindex
    IndexColumn column = 2;
}

message IndexProperties {
    oneof props {
        InvertedIndex inverted = 1;
        VectorIvfPqIndex vector = 2;
        BTreeIndex btree = 3;
    }
}

message InvertedIndex {
    bool store_position = 1;
    string base_tokenizer = 2;
    // TODO(zehiko) add other properties as needed
}

message VectorIvfPqIndex {
    uint32 num_partitions = 1;
    uint32 num_sub_vectors = 2;
    VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
    L2 = 0;
    COSINE = 1;
    DOT = 2;
    HAMMING = 3;
}

message BTreeIndex {
    // TODO(zehiko) add properties as needed
}

message CreateIndexResponse {
    uint64 indexed_rows = 1;
}

message ReIndexResponse {
}

// ---------------- SearchIndex ------------------

message SearchIndexRequest {
    // The catalog entry that we want to search over
    // If not specified, the default collection is queried
    CatalogEntry entry = 1;
    // Index column that is queried
    IndexColumn column = 2;
    // Query data - type of data is index specific. Caller must ensure
    // to provide the right type. For vector search this should
    // be a vector of appropriate size, for inverted index this should be a string.
    // Query data is represented as a unit (single row) RecordBatch with 1 column.
    DataframePart query = 3;
    // Index type specific properties
    IndexQueryProperties properties = 4;
    // max number of rows to be returned
    optional uint32 limit = 5;
}

message IndexQueryProperties {
    // specific index query properties based on the index type
    oneof props {
        InvertedIndexQuery inverted = 1;
        VectorIndexQuery vector = 2;
        BTreeIndexQuery btree = 3;
    }
}

message InvertedIndexQuery {
    // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
    uint32 top_k = 2;
}

message BTreeIndexQuery {
    // TODO(zehiko) add properties as needed
}

message CatalogEntry {
    string name = 1;
}

// ---------------- GetRecordingSchema ------------------

message GetRecordingSchemaRequest {
    rerun.common.v0.RecordingId recording_id = 1;
}

message GetRecordingSchemaResponse {
    rerun.common.v0.Schema schema = 2;
}

// ---------------- RegisterRecording ------------------

message RegisterRecordingRequest {
    // human readable description of the recording
    string description = 1;
    // recording storage url (e.g. s3://bucket/file or file:///path/to/file)
    string storage_url = 2;
    // type of recording
    RecordingType typ = 3;
    // (optional) any additional metadata that should be associated with the recording
    // You can associate any arbtrirary number of columns with a specific recording
    DataframePart metadata = 4;
}

// ---------------- Unregister from catalog ------------------

message UnregisterRecordingRequest {
    // unique identifier of the recording
    rerun.common.v0.RecordingId recording_id = 1;
}
message UnregisterRecordingResponse {}

message UnregisterAllRecordingsRequest {}
message UnregisterAllRecordingsResponse {}

// ---------------- UpdateCatalog  -----------------

message UpdateCatalogRequest {
    DataframePart metadata = 2;
}

message UpdateCatalogResponse {}

// ---------------- Query -----------------

message QueryRequest {
    // unique identifier of the recording
    rerun.common.v0.RecordingId recording_id = 1;
    // query to execute
    rerun.common.v0.Query query = 3;
}

// ----------------- QueryCatalog -----------------

message QueryCatalogRequest {
    // Column projection - define which columns should be returned.
    // Providing it is optional, if not provided, all columns should be returned
    ColumnProjection column_projection = 1;
    // Filter specific recordings that match the criteria (selection)
    CatalogFilter filter = 2;
}

message ColumnProjection {
    repeated string columns = 1;
}

message CatalogFilter {
    // Filtering is very simple right now, we can only select
    // recordings by their ids.
    repeated rerun.common.v0.RecordingId recording_ids = 1;
}

message QueryCatalogResponse {
    rerun.common.v0.EncoderVersion encoder_version = 1;
    // raw bytes are TransportChunks (i.e. RecordBatches) encoded with the relevant codec
    bytes payload = 2;
}

enum RecordingType {
    RRD = 0;
}

// ----------------- FetchRecording -----------------

message FetchRecordingRequest {
    rerun.common.v0.RecordingId recording_id = 1;
}

// TODO(jleibs): Eventually this becomes either query-mediated in some way, but for now
// it's useful to be able to just get back the whole RRD somehow.
message FetchRecordingResponse {
    // TODO(zehiko) we need to expand this to become something like 'encoder options'
    // as we will need to specify additional options like compression, including schema
    // in payload, etc.
    rerun.common.v0.EncoderVersion encoder_version = 1;
    // payload is raw bytes that the relevant codec can interpret
    bytes payload = 2;
}

// ----------------- Error handling -----------------

// Application level error - used as `details` in the `google.rpc.Status` message
message RemoteStoreError {
    // error code
    ErrorCode code = 1;
    // unique identifier associated with the request (e.g. recording id, recording storage url)
    string id = 2;
    // human readable details about the error
    string message = 3;
}

// Error codes for application level errors
enum ErrorCode {
    // unused
    _UNUSED = 0;

    // object store access error
    OBJECT_STORE_ERROR = 1;

    // metadata database access error
    METADATA_DB_ERROR = 2;

    // Encoding / decoding error
    CODEC_ERROR = 3;
}
